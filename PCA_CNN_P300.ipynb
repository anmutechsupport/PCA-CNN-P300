{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PCA-CNN P300",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anmutechsupport/PCA-CNN-P300/blob/main/PCA_CNN_P300.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnxigHwOJV3G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff84aa8d-29de-4d78-ef89-1b202a4101ac"
      },
      "source": [
        "pip install tensorflow_transform"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_transform\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/68/0c5e5148c13ef5d1897d396577183377839ec7b7b46670e3cc1be2e16e46/tensorflow_transform-1.0.0-py3-none-any.whl (402kB)\n",
            "\r\u001b[K     |▉                               | 10kB 19.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 28.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30kB 32.9MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40kB 21.1MB/s eta 0:00:01\r\u001b[K     |████                            | 51kB 16.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 61kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 71kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 81kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 92kB 11.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 102kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 122kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 133kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 143kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 153kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 163kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 174kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 184kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 194kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 204kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 215kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 225kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 235kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 245kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 256kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 266kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 276kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 286kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 296kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 307kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 317kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 327kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 337kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 348kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 358kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 368kB 12.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 378kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 389kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 399kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 409kB 12.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_transform) (2.5.0)\n",
            "Collecting tfx-bsl<1.1.0,>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/cb/1a2c5d380c6884fb522954a2615adf6f2fa5b5ac3f5af98c656c5488edd5/tfx_bsl-1.0.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 28.0MB/s \n",
            "\u001b[?25hCollecting apache-beam[gcp]<3,>=2.29\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/7f/342e6bf4bbdc55418c929b3281948070ef4ca83e198dc9135352c25799f9/apache_beam-2.29.0-cp37-cp37m-manylinux2010_x86_64.whl (9.6MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6MB 56.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow_transform) (1.15.0)\n",
            "Requirement already satisfied: absl-py<0.13,>=0.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow_transform) (0.12.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_transform) (1.3.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_transform) (3.12.4)\n",
            "Requirement already satisfied: numpy<1.20,>=1.16 in /usr/local/lib/python3.7/dist-packages (from tensorflow_transform) (1.19.5)\n",
            "Collecting pyarrow<3,>=1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/8d/c002e27767595f22aa09ed0d364327922f673d12b36526c967a2bf6b2ed7/pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 263kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-metadata<1.1.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_transform) (1.0.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (1.12.1)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (3.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (1.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (2.5.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (1.34.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (0.36.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (3.7.4.3)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<1.1.0,>=1.0.0->tensorflow_transform) (1.1.5)\n",
            "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15\n",
            "  Downloading https://files.pythonhosted.org/packages/27/a6/a534deae4086c0fef9a77537a4779bb5a9dd8841f8d347c509c96b342b2e/tensorflow_serving_api-2.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.11 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<1.1.0,>=1.0.0->tensorflow_transform) (1.12.8)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 58.2MB/s \n",
            "\u001b[?25hCollecting future<1.0.0,>=0.18.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow_transform) (1.7)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/08/f7/4c3fad73123a24d7394b6f40d1ec9c1cbf2e921cfea1797216ffd0a51fb1/hdfs-2.6.0-py3-none-any.whl\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/d1/8f5c8611026f0ddcd86a8e2f965998e0c159af980c31efba72342c69f3e4/fastavro-1.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 52.4MB/s \n",
            "\u001b[?25hCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/80/acd1455bea0a9fcdc60a748a97dcbb3ff624726fb90987a0fc1c19e7a5a5/avro-python3-1.9.2.1.tar.gz\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow_transform) (3.11.4)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow_transform) (4.1.3)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow_transform) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow_transform) (2018.9)\n",
            "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow_transform) (0.17.4)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow_transform) (4.2.2)\n",
            "Collecting google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/da/aefc4cf4c168b5d875344cd9dddc77e3a2d11986b630251af5ce47dd2843/google-apitools-0.5.31.tar.gz (173kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 60.7MB/s \n",
            "\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/7f/e10d602c2dc3f749f1b78377a3357790f1da71b28e7da9e5bc20b3a9bd40/google_cloud_vision-1.0.0-py2.py3-none-any.whl (435kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 70.9MB/s \n",
            "\u001b[?25hCollecting google-cloud-dlp<2,>=0.12.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/c3/5b73c15f59207b20df288573c2ea203c7b126df8330add380d8b50bc0d5c/google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 67.0MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/29/8d06211102c87768dc34943d9c92abd8b67491ffedd6d09b56305b1ab255/google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 75.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow_transform) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.18.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow_transform) (1.30.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow_transform) (1.21.0)\n",
            "Collecting grpcio-gcp<1,>=0.2.2; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/83/1f1095815be0de19102df41e250ebbd7dae97d7d14e22c18da07ed5ed9d4/grpcio_gcp-0.2.2-py2.py3-none-any.whl\n",
            "Collecting google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/b3/dd83eca4cd1019d592e82595ea45d53f11e39db4ee99daa66ceb8a1b2d89/google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 43.5MB/s \n",
            "\u001b[?25hCollecting google-cloud-language<2,>=1.3.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b8/965a97ba60287910d342623da1da615254bded3e0965728cf7fc6339b7c8/google_cloud_language-1.3.0-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.1MB/s \n",
            "\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/16/c9262ca40f3a278f38df9d21dece1ae01ee24f8ed29937bd1f066f908f9f/google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 59.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow_transform) (1.0.3)\n",
            "Collecting google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/17/536768bada8f93f124826b36dbdcdf08edd0e5ef0ca76b4c911f9f28596a/google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 63.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2->tensorflow_transform) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.9.2->tensorflow_transform) (56.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata<1.1.0,>=1.0.0->tensorflow_transform) (1.53.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (1.8.0)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.1.0,>=1.0.0->tensorflow_transform) (1.26.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.1.0,>=1.0.0->tensorflow_transform) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.1.0,>=1.0.0->tensorflow_transform) (3.0.1)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.29->tensorflow_transform) (0.6.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.29->tensorflow_transform) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.29->tensorflow_transform) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.29->tensorflow_transform) (0.2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.29->tensorflow_transform) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.29->tensorflow_transform) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.29->tensorflow_transform) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.29->tensorflow_transform) (2.10)\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/78/20/c862d765287e9e8b29f826749ebae8775bdca50b2cb2ca079346d5fbfd76/fasteners-0.16-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<3,>=2.29->tensorflow_transform) (0.4.1)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (4.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (1.3.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.11->tfx-bsl<1.1.0,>=1.0.0->tensorflow_transform) (20.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<2.6,>=1.15.2->tensorflow_transform) (3.1.0)\n",
            "Building wheels for collected packages: dill, future, avro-python3, google-apitools, grpc-google-iam-v1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-cp37-none-any.whl size=78532 sha256=c140022bbf65118ce8dbfb3ee11f2cf97709d5b8b0f24114bbcb4007ad73cf1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=bb19eb057ba4946f5de49966b4a9e67a48f61972e5ecf62f4f05961316209d71\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-cp37-none-any.whl size=43516 sha256=01465d60f7416ae895fde150f317a64b674940af4fd2f74e31731b9d02eb7698\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d3/be/86620c9dd3fca68986c33b9c616510289fc0abb81ec9aa70bd\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-cp37-none-any.whl size=131043 sha256=69a5ec2329c14f80a950f64efe71d0dd2a4b4fd3b9ae3e950b8bed96a435c0ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/43/31/09a9dad88d3aec6fed2d63bd35dfc532fca372e2edec5af5bf\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp37-none-any.whl size=18500 sha256=1c15e141454b17732f6fd21b5b432faf7d16ad2b4e906ff284c6541b19872e69\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "Successfully built dill future avro-python3 google-apitools grpc-google-iam-v1\n",
            "\u001b[31mERROR: multiprocess 0.70.11.1 has requirement dill>=0.3.3, but you'll have dill 0.3.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-spanner 1.19.1 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-bigtable 1.7.0 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: dill, future, requests, hdfs, fastavro, avro-python3, pyarrow, fasteners, google-apitools, google-cloud-vision, google-cloud-dlp, google-cloud-videointelligence, grpcio-gcp, grpc-google-iam-v1, google-cloud-pubsub, google-cloud-language, google-cloud-spanner, google-cloud-bigtable, apache-beam, tensorflow-serving-api, tfx-bsl, tensorflow-transform\n",
            "  Found existing installation: dill 0.3.3\n",
            "    Uninstalling dill-0.3.3:\n",
            "      Successfully uninstalled dill-0.3.3\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "Successfully installed apache-beam-2.29.0 avro-python3-1.9.2.1 dill-0.3.1.1 fastavro-1.4.1 fasteners-0.16 future-0.18.2 google-apitools-0.5.31 google-cloud-bigtable-1.7.0 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-pubsub-1.7.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 hdfs-2.6.0 pyarrow-2.0.0 requests-2.25.1 tensorflow-serving-api-2.5.1 tensorflow-transform-1.0.0 tfx-bsl-1.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iebh74wcPHx1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, concatenate, Conv3D\n",
        "from pprint import pprint\n",
        "# import tensorflow_transform as tft\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "from scipy.signal import butter, lfilter, freqz\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import seaborn as sns\n",
        "\n",
        "# import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xnQWfkpUyO-"
      },
      "source": [
        "# K.set_image_data_format('channels_last') #does not work\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.enable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTPEWYWVvJyF",
        "outputId": "a3f129f7-be96-4021-835f-07d811b8579f"
      },
      "source": [
        "def minibatch(data, batch_size):\n",
        "    start = 0\n",
        "    while True:\n",
        "\n",
        "        end = start + batch_size\n",
        "        yield data[start:end]\n",
        "\n",
        "        start = end\n",
        "        if start >= len(data):\n",
        "            break\n",
        "\n",
        "# function to collect matching files and dirs\n",
        "def collect_files(root, res, pattern=\"\", collect_dirs=True, min_depth=None, max_depth=None):\n",
        "    \n",
        "    # check max depth\n",
        "    if not max_depth is None and max_depth == 0:\n",
        "        return\n",
        "    \n",
        "    # go through all item in the dir\n",
        "    for item in os.listdir(root):\n",
        "        \n",
        "        # process item\n",
        "        item_path = os.path.join(root, item)\n",
        "        item_is_dir = os.path.isdir(item_path)\n",
        "        \n",
        "        # pull valid file in res if min depth has reached\n",
        "        if min_depth is None or min_depth - 1 <= 0:\n",
        "            if re.match(pattern, item_path):\n",
        "                if not item_is_dir or collect_dirs:\n",
        "                    res.append(item_path)\n",
        "        \n",
        "        # recursively collect all files\n",
        "        if item_is_dir:\n",
        "            next_min_depth = None if min_depth is None else min_depth - 1\n",
        "            next_max_depth = None if max_depth is None else max_depth - 1\n",
        "            collect_files(item_path, res, pattern, collect_dirs, next_min_depth, next_max_depth)\n",
        "\n",
        "# collect the mat files\n",
        "mat_files = []\n",
        "collect_files(\"./\", mat_files, pattern=\".*\\.mat$\", collect_dirs=False)\n",
        "mat_files.sort()\n",
        "print(mat_files)\n",
        "\n",
        "# load all data into memory\n",
        "# all_data[i] means data for ith subject\n",
        "all_data = []\n",
        "for i, mat_file in enumerate(mat_files):\n",
        "    \n",
        "    # re-index tmp into a dictionary\n",
        "    tmp = loadmat(mat_file)[\"data\"][0][0]\n",
        "    tmp = {name: data for name, data in zip(tmp.dtype.names, tmp)}\n",
        "    \n",
        "    # rename column\n",
        "    tmp[\"x\"] = tmp[\"X\"]\n",
        "    del tmp[\"X\"]\n",
        "    \n",
        "    # reshape columns\n",
        "    tmp[\"y\"] = tmp[\"y\"].reshape(-1)\n",
        "    tmp[\"y_stim\"] = tmp[\"y_stim\"].reshape(-1)\n",
        "    tmp[\"trial\"] = tmp[\"trial\"].reshape(-1)\n",
        "    \n",
        "    # add subject info\n",
        "    tmp[\"subject\"] = i + 1\n",
        "    \n",
        "    all_data.append(tmp)\n",
        "\n",
        "pprint(all_data[0])\n",
        "\n",
        "# constants for data_extraction\n",
        "sample_rate = 250 #hz\n",
        "tick_len = 1000 // sample_rate # ms -> 4\n",
        "pre_epoch = 0 #ms\n",
        "post_epoch = 700 #ms\n",
        "\n",
        "raw = all_data[0]['x']\n",
        "print(np.array(raw.shape))\n",
        "\n",
        "# give raw eeg data and tick times, return 2d signals\n",
        "def extract_epochs(raw, ticks):\n",
        "    pre_tick = int(pre_epoch // tick_len) # 0\n",
        "    post_tick = int(post_epoch // tick_len) #175\n",
        "    raw_len = len(raw) #350k+ datapoints -> each experiment is 1400s long -> sampling at 250hz \n",
        "    signals = [] \n",
        "    for t in ticks:\n",
        "        if t + post_tick <= raw_len: \n",
        "            signal = raw[t-pre_tick:t+post_tick, :] #takes slice of data points, takes all electrodes\n",
        "            signals.append(signal)\n",
        "    return np.array(signals) #1st dim == # of sample point ids\n",
        "\n",
        "# extract epochs for every subject\n",
        "for i, data in enumerate(all_data):\n",
        "    \n",
        "    # extract from raw\n",
        "    ticks, y_stim, y = data[\"flash\"][:, [0, 2, 3]].T #extracts sample point id, stimulation(0-12), hit/nohits(1/2)\n",
        "    raw = data[\"x\"]\n",
        "    \n",
        "    # get the epochs\n",
        "    epochs = extract_epochs(raw, ticks) #returning signals -> we only slice from sample point ids, why?\n",
        "    \n",
        "    # label the epochs\n",
        "    for j, x in enumerate(y):\n",
        "        assert x == 1 or x == 2\n",
        "        y[j] = 1 if x == 2 else 0 #switching labels from 1/2 to 0/1\n",
        "    \n",
        "    # trim extra y and y_stim\n",
        "    y = y[:len(epochs)] #takes all the labels until # of sample point ids\n",
        "    y_stim = y_stim[:len(epochs)] #takes all the stim until # of sample point ids\n",
        "    \n",
        "    assert len(epochs) == len(y) and len(y) == len(y_stim) # validating stuff that we did in the 2 lines above \n",
        "    \n",
        "    samples = np.array(list(zip(epochs, y, y_stim)), dtype=object) # concatenates everything into an np array\n",
        "    \n",
        "    # save the data\n",
        "    with open(f\"s{i+1}.pkl\", \"wb\") as outfile:\n",
        "        pickle.dump(samples, outfile) #saves sample np array into pkl\n",
        "\n",
        "print(samples.shape)\n",
        "\n",
        "with open(\"s7.pkl\", \"rb\") as infile:\n",
        "    data = pickle.load(infile)\n",
        "\n",
        "a = []\n",
        "for i in data[:, 0]: #appending data batches into 'a'\n",
        "    a.append(i)\n",
        "\n",
        "# print('a', np.array(a).shape)\n",
        "# a = np.array(a).reshape(4200, 8, 175)\n",
        "\n",
        "# for i in range(1, 9):\n",
        "#     with open(f\"s{i}.pkl\", \"rb\") as infile:\n",
        "#         data = pickle.load(infile)\n",
        "#         target = np.sum(data[:, 1]) #Adds all the labels together\n",
        "#         print(target, len(data) - target) #hits:nohits\n",
        "\n",
        "data_size = len(data)\n",
        "print([i for i in data[:, 1]]) #use these to decode with a simple if loop ->  not super sure if each stim represents one flash\n",
        "\n",
        "# shuffle data\n",
        "shuffle_idx = np.random.permutation(data_size)\n",
        "data = data[shuffle_idx]\n",
        "\n",
        "# 80-20 split train/test\n",
        "cutoff = int(data_size * 80 // 100)\n",
        "train_data = data[:cutoff]\n",
        "val_data = data[cutoff:]\n",
        "print('hello', train_data.shape)\n",
        "\n",
        "# balance label in the train_data\n",
        "train_data_size = len(train_data)\n",
        "train_data_true_count = np.sum([x[1] for x in train_data]) #taking sum of hit labels\n",
        "train_data_false_count = train_data_size - train_data_true_count #finding # of nohit labels\n",
        "\n",
        "assert train_data_false_count >= train_data_true_count \n",
        "\n",
        "train_data_dup_count = train_data_false_count - train_data_true_count \n",
        "train_data_true_idx = np.array([i for i, x in enumerate(train_data) if x[1] == 1])#index array of train data hit labels\n",
        "train_data_true_sample_idx = np.random.choice(train_data_true_idx, train_data_dup_count, replace=True) #creates random sample indices with and size of dup_count\n",
        "train_data_addon = train_data[train_data_true_sample_idx] #creates array using indices above\n",
        "\n",
        "# make sure that all the addon have true labels\n",
        "assert all([x[1] == 1 for x in train_data_addon])\n",
        "\n",
        "# stack the addon to the original trainning data and shuffle again\n",
        "train_data = np.concatenate((train_data, train_data_addon), axis=0)\n",
        "train_data_size = len(train_data)\n",
        "shuffle_idx = np.random.permutation(train_data_size)\n",
        "train_data = train_data[shuffle_idx] #final dataset\n",
        "print('hello', train_data.shape)\n",
        "\n",
        "'''\n",
        "'''\n",
        "\n",
        "# train_data = train_data[:(len(train_data)*10//100)]\n",
        "\n",
        "# data = np.array([i for i in data[:, 0]])\n",
        "# labels = np.array(data[:, 1])\n",
        "# stim = np.array(data[:, 2])\n",
        " \n",
        "T_data = np.array([i for i in train_data[:, 0]])\n",
        "T_labels = np.array([i for i in train_data[:, 1]])\n",
        "T_stim = np.array([i for i in train_data[:, 2]])\n",
        "\n",
        "V_data = np.array([i for i in val_data[:, 0]])\n",
        "V_labels = np.array([i for i in val_data[:, 1]])\n",
        "V_stim = np.array([i for i in val_data[:, 2]])\n",
        "\n",
        "print(len(data))\n",
        "print(V_labels.shape, T_data.shape)\n",
        "\n",
        "with open(f\"labels.pkl\", \"wb\") as outfile:\n",
        "  pickle.dump([T_labels, V_labels], outfile)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./P300S01.mat', './P300S02.mat', './P300S03.mat', './P300S04.mat', './P300S05.mat', './P300S06.mat', './P300S07.mat', './P300S08.mat']\n",
            "{'flash': array([[  7486,     31,     11,      1],\n",
            "       [  7548,     31,      5,      1],\n",
            "       [  7610,     30,      7,      1],\n",
            "       ...,\n",
            "       [355688,     32,      3,      2],\n",
            "       [355750,     32,      7,      1],\n",
            "       [355813,     32,      4,      1]], dtype=int32),\n",
            " 'subject': 1,\n",
            " 'trial': array([  7487,  17511,  27536,  37564,  47590,  57620,  67641,  77667,\n",
            "        87690,  97715, 107740, 117764, 127790, 137813, 147840, 157867,\n",
            "       167893, 177915, 187942, 197967, 207995, 218018, 228044, 238068,\n",
            "       248095, 258119, 268146, 278171, 288194, 298220, 308246, 318273,\n",
            "       328297, 338325, 348349], dtype=int32),\n",
            " 'x': array([[26.95640373, 11.73869324,  6.98503304, ...,  2.66381407,\n",
            "        -3.04957318, -2.63618731],\n",
            "       [29.94702721, 14.98267174, 10.02626705, ...,  5.16736984,\n",
            "         1.30663943,  2.16391516],\n",
            "       [33.09702683, 19.32771683, 13.60309792, ...,  8.05283546,\n",
            "         8.53473663,  7.90571976],\n",
            "       ...,\n",
            "       [ 5.6236558 ,  0.53022534,  1.55959177, ..., -4.96036482,\n",
            "         2.35364461, -8.1143961 ],\n",
            "       [ 4.38928461,  1.46526659,  1.96254373, ..., -4.6057415 ,\n",
            "         1.29674733, -7.18669939],\n",
            "       [ 2.58838463,  0.79275906,  0.29564747, ..., -4.98137999,\n",
            "        -4.04607677, -7.54975986]]),\n",
            " 'y': array([0, 0, 0, ..., 0, 0, 0], dtype=uint8),\n",
            " 'y_stim': array([0, 0, 0, ..., 0, 0, 0], dtype=uint8)}\n",
            "[358372      8]\n",
            "(4198, 3)\n",
            "[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "hello (3358, 3)\n",
            "hello (5610, 3)\n",
            "4198\n",
            "(840,) (5610, 175, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbaJ50AMupCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4e3b4f-d087-4e06-fcb1-f26e05adc55b"
      },
      "source": [
        "def standardize_data(arr):\n",
        "         \n",
        "    '''\n",
        "    This function standardize an array, its substracts mean value, \n",
        "    and then divide the standard deviation.\n",
        "    \n",
        "    param 1: array \n",
        "    return: standardized array\n",
        "    '''    \n",
        "    rows, columns = arr.shape\n",
        "    \n",
        "    standardizedArray = np.zeros(shape=(rows, columns)) \n",
        "    tempArray = np.zeros(rows)  \n",
        "    \n",
        "    for column in tqdm(range(columns)):\n",
        "        \n",
        "        # print(column)\n",
        "        mean = np.mean(X[:,column]) #mean of channel\n",
        "        std = np.std(X[:,column]) #std of channel\n",
        "        tempArray = np.empty(0)\n",
        "        \n",
        "        for element in X[:,column]: \n",
        "            \n",
        "            tempArray = np.append(tempArray, ((element - mean) / std)) #row val - mean/std = mean of 0, STD of 1\n",
        " \n",
        "        standardizedArray[:,column] = tempArray\n",
        "    \n",
        "    return standardizedArray\n",
        "\n",
        "ink = np.random.randn(50,64,240) #numpy\n",
        "\n",
        "PCA_array = [[], []]\n",
        "datax = [T_data, V_data]\n",
        "\n",
        "for data in tqdm(datax):\n",
        "# Standardizing data\n",
        "  X = data.reshape(len(data)*8, 175) #flatten axis for standardizing\n",
        "  X = standardize_data(X).reshape(-1, 8, 175)\n",
        "  # print(X.shape)\n",
        "\n",
        "  for batch in X:\n",
        "    # Calculating the covariance matrix\n",
        "    covariance_matrix = np.cov(batch.T)\n",
        "    # print(covariance_matrix)\n",
        "\n",
        "    # Using np.linalg.eig function\n",
        "    eigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)\n",
        "    # print(\"Eigenvector: \\n\",eigen_vectors,\"\\n\")\n",
        "    # print(\"Eigenvalues: \\n\", eigen_values, \"\\n\")\n",
        "\n",
        "  # Calculating the explained variance on each of components\n",
        "    variance_explained = []\n",
        "    for i in eigen_values:\n",
        "        variance_explained.append((i/sum(eigen_values))*100)\n",
        "          \n",
        "    # print(variance_explained)\n",
        "\n",
        "  # Identifying components that explain at least 95%\n",
        "\n",
        "    cumulative_variance_explained = np.cumsum(variance_explained)\n",
        "    # print(cumulative_variance_explained)\n",
        "\n",
        "  # Using two first components (because those explain more than 95%)\n",
        "    projection_matrix = (eigen_vectors.T[:][:60]).T\n",
        "    # print(projection_matrix)\n",
        "\n",
        "  # Getting the product of original standardized X and the eigenvectors \n",
        "    batch_pca = batch.dot(projection_matrix)\n",
        "    PCA_array[datax.index(data)].append(batch_pca)\n",
        "\n",
        "# print(PCA_array)\n",
        "PCA_array = np.array([np.array(x) for x in PCA_array], dtype=object)\n",
        "print(PCA_array.shape)\n",
        "with open(f\"PCA1.pkl\", \"wb\") as outfile:\n",
        "  pickle.dump(PCA_array, outfile)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]\n",
            "  0%|          | 0/175 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/175 [00:01<02:59,  1.03s/it]\u001b[A\n",
            "  1%|          | 2/175 [00:02<02:55,  1.01s/it]\u001b[A\n",
            "  2%|▏         | 3/175 [00:02<02:53,  1.01s/it]\u001b[A\n",
            "  2%|▏         | 4/175 [00:03<02:49,  1.01it/s]\u001b[A\n",
            "  3%|▎         | 5/175 [00:04<02:46,  1.02it/s]\u001b[A\n",
            "  3%|▎         | 6/175 [00:05<02:43,  1.03it/s]\u001b[A\n",
            "  4%|▍         | 7/175 [00:06<02:40,  1.04it/s]\u001b[A\n",
            "  5%|▍         | 8/175 [00:07<02:39,  1.05it/s]\u001b[A\n",
            "  5%|▌         | 9/175 [00:08<02:38,  1.05it/s]\u001b[A\n",
            "  6%|▌         | 10/175 [00:09<02:39,  1.03it/s]\u001b[A\n",
            "  6%|▋         | 11/175 [00:10<02:37,  1.04it/s]\u001b[A\n",
            "  7%|▋         | 12/175 [00:11<02:34,  1.05it/s]\u001b[A\n",
            "  7%|▋         | 13/175 [00:12<02:34,  1.05it/s]\u001b[A\n",
            "  8%|▊         | 14/175 [00:13<02:33,  1.05it/s]\u001b[A\n",
            "  9%|▊         | 15/175 [00:14<02:32,  1.05it/s]\u001b[A\n",
            "  9%|▉         | 16/175 [00:15<02:31,  1.05it/s]\u001b[A\n",
            " 10%|▉         | 17/175 [00:16<02:30,  1.05it/s]\u001b[A\n",
            " 10%|█         | 18/175 [00:17<02:30,  1.05it/s]\u001b[A\n",
            " 11%|█         | 19/175 [00:18<02:27,  1.06it/s]\u001b[A\n",
            " 11%|█▏        | 20/175 [00:19<02:26,  1.06it/s]\u001b[A\n",
            " 12%|█▏        | 21/175 [00:20<02:25,  1.06it/s]\u001b[A\n",
            " 13%|█▎        | 22/175 [00:21<02:24,  1.06it/s]\u001b[A\n",
            " 13%|█▎        | 23/175 [00:21<02:24,  1.06it/s]\u001b[A\n",
            " 14%|█▎        | 24/175 [00:22<02:23,  1.06it/s]\u001b[A\n",
            " 14%|█▍        | 25/175 [00:23<02:23,  1.05it/s]\u001b[A\n",
            " 15%|█▍        | 26/175 [00:24<02:21,  1.05it/s]\u001b[A\n",
            " 15%|█▌        | 27/175 [00:25<02:21,  1.05it/s]\u001b[A\n",
            " 16%|█▌        | 28/175 [00:26<02:20,  1.05it/s]\u001b[A\n",
            " 17%|█▋        | 29/175 [00:27<02:18,  1.05it/s]\u001b[A\n",
            " 17%|█▋        | 30/175 [00:28<02:19,  1.04it/s]\u001b[A\n",
            " 18%|█▊        | 31/175 [00:29<02:28,  1.03s/it]\u001b[A\n",
            " 18%|█▊        | 32/175 [00:31<02:34,  1.08s/it]\u001b[A\n",
            " 19%|█▉        | 33/175 [00:32<02:38,  1.12s/it]\u001b[A\n",
            " 19%|█▉        | 34/175 [00:33<02:40,  1.14s/it]\u001b[A\n",
            " 20%|██        | 35/175 [00:34<02:41,  1.15s/it]\u001b[A\n",
            " 21%|██        | 36/175 [00:35<02:42,  1.17s/it]\u001b[A\n",
            " 21%|██        | 37/175 [00:37<02:42,  1.18s/it]\u001b[A\n",
            " 22%|██▏       | 38/175 [00:38<02:41,  1.18s/it]\u001b[A\n",
            " 22%|██▏       | 39/175 [00:39<02:41,  1.19s/it]\u001b[A\n",
            " 23%|██▎       | 40/175 [00:40<02:40,  1.19s/it]\u001b[A\n",
            " 23%|██▎       | 41/175 [00:41<02:39,  1.19s/it]\u001b[A\n",
            " 24%|██▍       | 42/175 [00:43<02:39,  1.20s/it]\u001b[A\n",
            " 25%|██▍       | 43/175 [00:44<02:36,  1.19s/it]\u001b[A\n",
            " 25%|██▌       | 44/175 [00:45<02:35,  1.19s/it]\u001b[A\n",
            " 26%|██▌       | 45/175 [00:46<02:49,  1.30s/it]\u001b[A\n",
            " 26%|██▋       | 46/175 [00:48<02:56,  1.37s/it]\u001b[A\n",
            " 27%|██▋       | 47/175 [00:50<02:59,  1.41s/it]\u001b[A\n",
            " 27%|██▋       | 48/175 [00:51<03:02,  1.44s/it]\u001b[A\n",
            " 28%|██▊       | 49/175 [00:53<03:03,  1.46s/it]\u001b[A\n",
            " 29%|██▊       | 50/175 [00:54<03:03,  1.47s/it]\u001b[A\n",
            " 29%|██▉       | 51/175 [00:55<03:02,  1.47s/it]\u001b[A\n",
            " 30%|██▉       | 52/175 [00:57<03:01,  1.48s/it]\u001b[A\n",
            " 30%|███       | 53/175 [00:59<03:01,  1.49s/it]\u001b[A\n",
            " 31%|███       | 54/175 [01:00<03:01,  1.50s/it]\u001b[A\n",
            " 31%|███▏      | 55/175 [01:02<02:59,  1.50s/it]\u001b[A\n",
            " 32%|███▏      | 56/175 [01:03<02:56,  1.49s/it]\u001b[A\n",
            " 33%|███▎      | 57/175 [01:04<02:55,  1.49s/it]\u001b[A\n",
            " 33%|███▎      | 58/175 [01:06<02:54,  1.49s/it]\u001b[A\n",
            " 34%|███▎      | 59/175 [01:08<02:55,  1.51s/it]\u001b[A\n",
            " 34%|███▍      | 60/175 [01:09<02:55,  1.52s/it]\u001b[A\n",
            " 35%|███▍      | 61/175 [01:11<02:53,  1.53s/it]\u001b[A\n",
            " 35%|███▌      | 62/175 [01:12<02:51,  1.52s/it]\u001b[A\n",
            " 36%|███▌      | 63/175 [01:14<02:50,  1.52s/it]\u001b[A\n",
            " 37%|███▋      | 64/175 [01:15<02:48,  1.52s/it]\u001b[A\n",
            " 37%|███▋      | 65/175 [01:17<02:45,  1.51s/it]\u001b[A\n",
            " 38%|███▊      | 66/175 [01:18<02:43,  1.50s/it]\u001b[A\n",
            " 38%|███▊      | 67/175 [01:20<02:43,  1.51s/it]\u001b[A\n",
            " 39%|███▉      | 68/175 [01:21<02:40,  1.50s/it]\u001b[A\n",
            " 39%|███▉      | 69/175 [01:23<02:39,  1.50s/it]\u001b[A\n",
            " 40%|████      | 70/175 [01:24<02:37,  1.50s/it]\u001b[A\n",
            " 41%|████      | 71/175 [01:26<02:36,  1.50s/it]\u001b[A\n",
            " 41%|████      | 72/175 [01:27<02:34,  1.50s/it]\u001b[A\n",
            " 42%|████▏     | 73/175 [01:29<02:34,  1.51s/it]\u001b[A\n",
            " 42%|████▏     | 74/175 [01:30<02:31,  1.50s/it]\u001b[A\n",
            " 43%|████▎     | 75/175 [01:32<02:30,  1.50s/it]\u001b[A\n",
            " 43%|████▎     | 76/175 [01:33<02:29,  1.51s/it]\u001b[A\n",
            " 44%|████▍     | 77/175 [01:35<02:27,  1.51s/it]\u001b[A\n",
            " 45%|████▍     | 78/175 [01:36<02:26,  1.51s/it]\u001b[A\n",
            " 45%|████▌     | 79/175 [01:38<02:24,  1.51s/it]\u001b[A\n",
            " 46%|████▌     | 80/175 [01:39<02:22,  1.50s/it]\u001b[A\n",
            " 46%|████▋     | 81/175 [01:41<02:23,  1.52s/it]\u001b[A\n",
            " 47%|████▋     | 82/175 [01:42<02:20,  1.51s/it]\u001b[A\n",
            " 47%|████▋     | 83/175 [01:44<02:18,  1.51s/it]\u001b[A\n",
            " 48%|████▊     | 84/175 [01:45<02:15,  1.49s/it]\u001b[A\n",
            " 49%|████▊     | 85/175 [01:47<02:13,  1.48s/it]\u001b[A\n",
            " 49%|████▉     | 86/175 [01:48<02:11,  1.48s/it]\u001b[A\n",
            " 50%|████▉     | 87/175 [01:50<02:09,  1.48s/it]\u001b[A\n",
            " 50%|█████     | 88/175 [01:51<02:07,  1.47s/it]\u001b[A\n",
            " 51%|█████     | 89/175 [01:53<02:06,  1.47s/it]\u001b[A\n",
            " 51%|█████▏    | 90/175 [01:54<02:05,  1.47s/it]\u001b[A\n",
            " 52%|█████▏    | 91/175 [01:55<02:03,  1.47s/it]\u001b[A\n",
            " 53%|█████▎    | 92/175 [01:57<02:01,  1.46s/it]\u001b[A\n",
            " 53%|█████▎    | 93/175 [01:58<02:00,  1.47s/it]\u001b[A\n",
            " 54%|█████▎    | 94/175 [02:00<01:58,  1.47s/it]\u001b[A\n",
            " 54%|█████▍    | 95/175 [02:01<01:57,  1.47s/it]\u001b[A\n",
            " 55%|█████▍    | 96/175 [02:03<01:56,  1.48s/it]\u001b[A\n",
            " 55%|█████▌    | 97/175 [02:04<01:56,  1.49s/it]\u001b[A\n",
            " 56%|█████▌    | 98/175 [02:06<01:55,  1.50s/it]\u001b[A\n",
            " 57%|█████▋    | 99/175 [02:07<01:54,  1.51s/it]\u001b[A\n",
            " 57%|█████▋    | 100/175 [02:09<01:53,  1.51s/it]\u001b[A\n",
            " 58%|█████▊    | 101/175 [02:10<01:52,  1.52s/it]\u001b[A\n",
            " 58%|█████▊    | 102/175 [02:12<01:51,  1.53s/it]\u001b[A\n",
            " 59%|█████▉    | 103/175 [02:14<01:50,  1.53s/it]\u001b[A\n",
            " 59%|█████▉    | 104/175 [02:15<01:47,  1.51s/it]\u001b[A\n",
            " 60%|██████    | 105/175 [02:17<01:46,  1.52s/it]\u001b[A\n",
            " 61%|██████    | 106/175 [02:18<01:44,  1.51s/it]\u001b[A\n",
            " 61%|██████    | 107/175 [02:20<01:42,  1.51s/it]\u001b[A\n",
            " 62%|██████▏   | 108/175 [02:21<01:41,  1.51s/it]\u001b[A\n",
            " 62%|██████▏   | 109/175 [02:23<01:40,  1.52s/it]\u001b[A\n",
            " 63%|██████▎   | 110/175 [02:24<01:38,  1.52s/it]\u001b[A\n",
            " 63%|██████▎   | 111/175 [02:26<01:36,  1.50s/it]\u001b[A\n",
            " 64%|██████▍   | 112/175 [02:27<01:34,  1.49s/it]\u001b[A\n",
            " 65%|██████▍   | 113/175 [02:29<01:32,  1.49s/it]\u001b[A\n",
            " 65%|██████▌   | 114/175 [02:30<01:30,  1.48s/it]\u001b[A\n",
            " 66%|██████▌   | 115/175 [02:31<01:28,  1.48s/it]\u001b[A\n",
            " 66%|██████▋   | 116/175 [02:33<01:27,  1.48s/it]\u001b[A\n",
            " 67%|██████▋   | 117/175 [02:34<01:26,  1.49s/it]\u001b[A\n",
            " 67%|██████▋   | 118/175 [02:36<01:24,  1.49s/it]\u001b[A\n",
            " 68%|██████▊   | 119/175 [02:37<01:22,  1.47s/it]\u001b[A\n",
            " 69%|██████▊   | 120/175 [02:39<01:21,  1.48s/it]\u001b[A\n",
            " 69%|██████▉   | 121/175 [02:40<01:19,  1.48s/it]\u001b[A\n",
            " 70%|██████▉   | 122/175 [02:42<01:18,  1.48s/it]\u001b[A\n",
            " 70%|███████   | 123/175 [02:43<01:16,  1.47s/it]\u001b[A\n",
            " 71%|███████   | 124/175 [02:45<01:15,  1.48s/it]\u001b[A\n",
            " 71%|███████▏  | 125/175 [02:46<01:13,  1.48s/it]\u001b[A\n",
            " 72%|███████▏  | 126/175 [02:48<01:12,  1.49s/it]\u001b[A\n",
            " 73%|███████▎  | 127/175 [02:49<01:11,  1.50s/it]\u001b[A\n",
            " 73%|███████▎  | 128/175 [02:51<01:10,  1.49s/it]\u001b[A\n",
            " 74%|███████▎  | 129/175 [02:52<01:08,  1.49s/it]\u001b[A\n",
            " 74%|███████▍  | 130/175 [02:54<01:07,  1.50s/it]\u001b[A\n",
            " 75%|███████▍  | 131/175 [02:55<01:05,  1.49s/it]\u001b[A\n",
            " 75%|███████▌  | 132/175 [02:57<01:03,  1.49s/it]\u001b[A\n",
            " 76%|███████▌  | 133/175 [02:58<01:02,  1.49s/it]\u001b[A\n",
            " 77%|███████▋  | 134/175 [03:00<01:00,  1.48s/it]\u001b[A\n",
            " 77%|███████▋  | 135/175 [03:01<00:59,  1.50s/it]\u001b[A\n",
            " 78%|███████▊  | 136/175 [03:03<00:58,  1.50s/it]\u001b[A\n",
            " 78%|███████▊  | 137/175 [03:04<00:56,  1.50s/it]\u001b[A\n",
            " 79%|███████▉  | 138/175 [03:06<00:55,  1.50s/it]\u001b[A\n",
            " 79%|███████▉  | 139/175 [03:07<00:53,  1.50s/it]\u001b[A\n",
            " 80%|████████  | 140/175 [03:09<00:52,  1.50s/it]\u001b[A\n",
            " 81%|████████  | 141/175 [03:10<00:50,  1.49s/it]\u001b[A\n",
            " 81%|████████  | 142/175 [03:12<00:49,  1.49s/it]\u001b[A\n",
            " 82%|████████▏ | 143/175 [03:13<00:47,  1.49s/it]\u001b[A\n",
            " 82%|████████▏ | 144/175 [03:15<00:46,  1.49s/it]\u001b[A\n",
            " 83%|████████▎ | 145/175 [03:16<00:44,  1.48s/it]\u001b[A\n",
            " 83%|████████▎ | 146/175 [03:18<00:42,  1.48s/it]\u001b[A\n",
            " 84%|████████▍ | 147/175 [03:19<00:41,  1.48s/it]\u001b[A\n",
            " 85%|████████▍ | 148/175 [03:21<00:39,  1.48s/it]\u001b[A\n",
            " 85%|████████▌ | 149/175 [03:22<00:38,  1.49s/it]\u001b[A\n",
            " 86%|████████▌ | 150/175 [03:24<00:36,  1.47s/it]\u001b[A\n",
            " 86%|████████▋ | 151/175 [03:25<00:35,  1.48s/it]\u001b[A\n",
            " 87%|████████▋ | 152/175 [03:26<00:33,  1.47s/it]\u001b[A\n",
            " 87%|████████▋ | 153/175 [03:28<00:32,  1.47s/it]\u001b[A\n",
            " 88%|████████▊ | 154/175 [03:29<00:31,  1.48s/it]\u001b[A\n",
            " 89%|████████▊ | 155/175 [03:31<00:29,  1.48s/it]\u001b[A\n",
            " 89%|████████▉ | 156/175 [03:32<00:28,  1.47s/it]\u001b[A\n",
            " 90%|████████▉ | 157/175 [03:34<00:26,  1.47s/it]\u001b[A\n",
            " 90%|█████████ | 158/175 [03:35<00:25,  1.47s/it]\u001b[A\n",
            " 91%|█████████ | 159/175 [03:37<00:23,  1.48s/it]\u001b[A\n",
            " 91%|█████████▏| 160/175 [03:38<00:22,  1.47s/it]\u001b[A\n",
            " 92%|█████████▏| 161/175 [03:40<00:20,  1.47s/it]\u001b[A\n",
            " 93%|█████████▎| 162/175 [03:41<00:19,  1.48s/it]\u001b[A\n",
            " 93%|█████████▎| 163/175 [03:43<00:17,  1.48s/it]\u001b[A\n",
            " 94%|█████████▎| 164/175 [03:44<00:16,  1.49s/it]\u001b[A\n",
            " 94%|█████████▍| 165/175 [03:46<00:14,  1.49s/it]\u001b[A\n",
            " 95%|█████████▍| 166/175 [03:47<00:13,  1.49s/it]\u001b[A\n",
            " 95%|█████████▌| 167/175 [03:49<00:11,  1.48s/it]\u001b[A\n",
            " 96%|█████████▌| 168/175 [03:50<00:10,  1.48s/it]\u001b[A\n",
            " 97%|█████████▋| 169/175 [03:52<00:08,  1.47s/it]\u001b[A\n",
            " 97%|█████████▋| 170/175 [03:53<00:07,  1.47s/it]\u001b[A\n",
            " 98%|█████████▊| 171/175 [03:55<00:05,  1.47s/it]\u001b[A\n",
            " 98%|█████████▊| 172/175 [03:56<00:04,  1.47s/it]\u001b[A\n",
            " 99%|█████████▉| 173/175 [03:57<00:02,  1.45s/it]\u001b[A\n",
            " 99%|█████████▉| 174/175 [03:59<00:01,  1.45s/it]\u001b[A\n",
            "100%|██████████| 175/175 [04:00<00:00,  1.38s/it]\n",
            " 50%|█████     | 1/2 [08:50<08:50, 530.40s/it]\n",
            "  0%|          | 0/175 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/175 [00:00<00:20,  8.53it/s]\u001b[A\n",
            "  2%|▏         | 3/175 [00:00<00:17,  9.70it/s]\u001b[A\n",
            "  3%|▎         | 5/175 [00:00<00:16, 10.36it/s]\u001b[A\n",
            "  4%|▍         | 7/175 [00:00<00:14, 11.65it/s]\u001b[A\n",
            "  5%|▌         | 9/175 [00:00<00:12, 12.94it/s]\u001b[A\n",
            "  6%|▋         | 11/175 [00:00<00:11, 13.87it/s]\u001b[A\n",
            "  7%|▋         | 13/175 [00:00<00:11, 14.61it/s]\u001b[A\n",
            "  9%|▊         | 15/175 [00:01<00:10, 14.89it/s]\u001b[A\n",
            " 10%|▉         | 17/175 [00:01<00:10, 15.10it/s]\u001b[A\n",
            " 11%|█         | 19/175 [00:01<00:10, 14.77it/s]\u001b[A\n",
            " 12%|█▏        | 21/175 [00:01<00:09, 15.43it/s]\u001b[A\n",
            " 13%|█▎        | 23/175 [00:01<00:09, 15.69it/s]\u001b[A\n",
            " 14%|█▍        | 25/175 [00:01<00:09, 16.13it/s]\u001b[A\n",
            " 15%|█▌        | 27/175 [00:01<00:09, 15.91it/s]\u001b[A\n",
            " 17%|█▋        | 29/175 [00:01<00:09, 15.47it/s]\u001b[A\n",
            " 18%|█▊        | 31/175 [00:02<00:09, 15.42it/s]\u001b[A\n",
            " 19%|█▉        | 33/175 [00:02<00:09, 14.98it/s]\u001b[A\n",
            " 20%|██        | 35/175 [00:02<00:09, 14.56it/s]\u001b[A\n",
            " 21%|██        | 37/175 [00:02<00:09, 14.21it/s]\u001b[A\n",
            " 22%|██▏       | 39/175 [00:02<00:09, 14.32it/s]\u001b[A\n",
            " 23%|██▎       | 41/175 [00:02<00:09, 14.77it/s]\u001b[A\n",
            " 25%|██▍       | 43/175 [00:02<00:08, 15.46it/s]\u001b[A\n",
            " 26%|██▌       | 45/175 [00:02<00:08, 15.96it/s]\u001b[A\n",
            " 27%|██▋       | 47/175 [00:03<00:07, 16.19it/s]\u001b[A\n",
            " 28%|██▊       | 49/175 [00:03<00:07, 16.18it/s]\u001b[A\n",
            " 29%|██▉       | 51/175 [00:03<00:07, 16.11it/s]\u001b[A\n",
            " 30%|███       | 53/175 [00:03<00:07, 15.88it/s]\u001b[A\n",
            " 31%|███▏      | 55/175 [00:03<00:07, 16.04it/s]\u001b[A\n",
            " 33%|███▎      | 57/175 [00:03<00:07, 15.59it/s]\u001b[A\n",
            " 34%|███▎      | 59/175 [00:03<00:07, 15.37it/s]\u001b[A\n",
            " 35%|███▍      | 61/175 [00:03<00:07, 15.56it/s]\u001b[A\n",
            " 36%|███▌      | 63/175 [00:04<00:07, 15.64it/s]\u001b[A\n",
            " 37%|███▋      | 65/175 [00:04<00:06, 15.82it/s]\u001b[A\n",
            " 38%|███▊      | 67/175 [00:04<00:06, 15.50it/s]\u001b[A\n",
            " 39%|███▉      | 69/175 [00:04<00:06, 15.92it/s]\u001b[A\n",
            " 41%|████      | 71/175 [00:04<00:06, 16.08it/s]\u001b[A\n",
            " 42%|████▏     | 73/175 [00:04<00:06, 15.61it/s]\u001b[A\n",
            " 43%|████▎     | 75/175 [00:04<00:06, 15.92it/s]\u001b[A\n",
            " 44%|████▍     | 77/175 [00:05<00:06, 15.81it/s]\u001b[A\n",
            " 45%|████▌     | 79/175 [00:05<00:06, 15.88it/s]\u001b[A\n",
            " 46%|████▋     | 81/175 [00:05<00:05, 16.16it/s]\u001b[A\n",
            " 47%|████▋     | 83/175 [00:05<00:05, 16.17it/s]\u001b[A\n",
            " 49%|████▊     | 85/175 [00:05<00:05, 16.08it/s]\u001b[A\n",
            " 50%|████▉     | 87/175 [00:05<00:05, 15.40it/s]\u001b[A\n",
            " 51%|█████     | 89/175 [00:05<00:05, 15.41it/s]\u001b[A\n",
            " 52%|█████▏    | 91/175 [00:05<00:05, 15.39it/s]\u001b[A\n",
            " 53%|█████▎    | 93/175 [00:06<00:05, 15.74it/s]\u001b[A\n",
            " 54%|█████▍    | 95/175 [00:06<00:05, 15.68it/s]\u001b[A\n",
            " 55%|█████▌    | 97/175 [00:06<00:05, 15.52it/s]\u001b[A\n",
            " 57%|█████▋    | 99/175 [00:06<00:04, 15.81it/s]\u001b[A\n",
            " 58%|█████▊    | 101/175 [00:06<00:04, 15.93it/s]\u001b[A\n",
            " 59%|█████▉    | 103/175 [00:06<00:04, 16.23it/s]\u001b[A\n",
            " 60%|██████    | 105/175 [00:06<00:04, 16.51it/s]\u001b[A\n",
            " 61%|██████    | 107/175 [00:06<00:04, 16.75it/s]\u001b[A\n",
            " 62%|██████▏   | 109/175 [00:06<00:03, 16.90it/s]\u001b[A\n",
            " 63%|██████▎   | 111/175 [00:07<00:03, 17.01it/s]\u001b[A\n",
            " 65%|██████▍   | 113/175 [00:07<00:03, 16.87it/s]\u001b[A\n",
            " 66%|██████▌   | 115/175 [00:07<00:03, 16.33it/s]\u001b[A\n",
            " 67%|██████▋   | 117/175 [00:07<00:03, 16.16it/s]\u001b[A\n",
            " 68%|██████▊   | 119/175 [00:07<00:03, 15.96it/s]\u001b[A\n",
            " 69%|██████▉   | 121/175 [00:07<00:03, 15.64it/s]\u001b[A\n",
            " 70%|███████   | 123/175 [00:07<00:03, 16.02it/s]\u001b[A\n",
            " 71%|███████▏  | 125/175 [00:07<00:03, 16.32it/s]\u001b[A\n",
            " 73%|███████▎  | 127/175 [00:08<00:02, 16.77it/s]\u001b[A\n",
            " 74%|███████▎  | 129/175 [00:08<00:02, 16.79it/s]\u001b[A\n",
            " 75%|███████▍  | 131/175 [00:08<00:02, 16.28it/s]\u001b[A\n",
            " 76%|███████▌  | 133/175 [00:08<00:02, 16.40it/s]\u001b[A\n",
            " 77%|███████▋  | 135/175 [00:08<00:02, 16.54it/s]\u001b[A\n",
            " 78%|███████▊  | 137/175 [00:08<00:02, 16.89it/s]\u001b[A\n",
            " 79%|███████▉  | 139/175 [00:08<00:02, 16.95it/s]\u001b[A\n",
            " 81%|████████  | 141/175 [00:08<00:01, 17.01it/s]\u001b[A\n",
            " 82%|████████▏ | 143/175 [00:09<00:01, 17.01it/s]\u001b[A\n",
            " 83%|████████▎ | 145/175 [00:09<00:01, 17.09it/s]\u001b[A\n",
            " 84%|████████▍ | 147/175 [00:09<00:01, 16.33it/s]\u001b[A\n",
            " 85%|████████▌ | 149/175 [00:09<00:01, 16.35it/s]\u001b[A\n",
            " 86%|████████▋ | 151/175 [00:09<00:01, 16.51it/s]\u001b[A\n",
            " 87%|████████▋ | 153/175 [00:09<00:01, 16.66it/s]\u001b[A\n",
            " 89%|████████▊ | 155/175 [00:09<00:01, 16.43it/s]\u001b[A\n",
            " 90%|████████▉ | 157/175 [00:09<00:01, 16.51it/s]\u001b[A\n",
            " 91%|█████████ | 159/175 [00:10<00:00, 16.77it/s]\u001b[A\n",
            " 92%|█████████▏| 161/175 [00:10<00:00, 16.95it/s]\u001b[A\n",
            " 93%|█████████▎| 163/175 [00:10<00:00, 16.91it/s]\u001b[A\n",
            " 94%|█████████▍| 165/175 [00:10<00:00, 16.56it/s]\u001b[A\n",
            " 95%|█████████▌| 167/175 [00:10<00:00, 16.32it/s]\u001b[A\n",
            " 97%|█████████▋| 169/175 [00:10<00:00, 16.08it/s]\u001b[A\n",
            " 98%|█████████▊| 171/175 [00:10<00:00, 15.70it/s]\u001b[A\n",
            " 99%|█████████▉| 173/175 [00:10<00:00, 16.24it/s]\u001b[A\n",
            "100%|██████████| 175/175 [00:11<00:00, 15.91it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "100%|██████████| 2/2 [09:43<00:00, 291.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(2,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJa3YkCyEc9W",
        "outputId": "4e3ad7ac-c2db-4506-dce3-1c430f4b13bc"
      },
      "source": [
        "with open(\"PCA1.pkl\", \"rb\") as infile:\n",
        "    pca = pickle.load(infile)\n",
        "\n",
        "print(pca[1].shape)\n",
        "\n",
        "#hold out split\n",
        "PCA_val, PCA_train = pca[1].reshape(-1, 8, 60, 1),  pca[0].reshape(-1, 8, 60, 1)\n",
        "\n",
        "PCA_train.shape\n",
        "\n",
        "\n",
        "# print(PCA_val.shape, PCA_train.shape, labels_val.shape, labels_train.shape, labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(840, 8, 60)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5610, 8, 60, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiwW-p2w6kqF",
        "outputId": "6cf071f3-1ac0-4cb0-988b-e20a73797792"
      },
      "source": [
        "with open(\"labels.pkl\", \"rb\") as infile:\n",
        "    T_labels, V_labels = pickle.load(infile)\n",
        "\n",
        "print(T_labels.shape)\n",
        "print(V_labels.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5610,)\n",
            "(840,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGWQ29k_Q1w4"
      },
      "source": [
        "120 dim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_E92qlePUkp",
        "outputId": "46699515-a617-454d-8b3d-b751254a6220"
      },
      "source": [
        "model = models.Sequential()\n",
        "inp_conc1 = Input(shape=(1, 120, 20))\n",
        "inp_conc2 = Input(shape=(1, 44, 16))\n",
        "\n",
        "#l2\n",
        "model.add(Conv2D(20, (8, 1), activation=\"relu\", strides=(8, 1), padding='same', input_shape=(8, 120, 1))) #just change kernel size here and you should be good to go with any array\n",
        "# print(L2.shape) #0th index: # of batches, 1st index: height of output, 2nd index: width of output, 3rd index: depth -> # of filters\n",
        "#l3\n",
        "L3_1 = Conv2D(filters=16, kernel_size=(1, 5), strides=(1, 5), padding=\"same\", activation=\"relu\", input_shape=(1, 120, 20))(inp_conc1)\n",
        "# print(L3_1.shape)\n",
        "\n",
        "L3_2 = Conv2D(filters=16, kernel_size=(1, 10), strides=(1, 10), padding=\"same\", activation=\"relu\", input_shape=(1, 120, 20))(inp_conc1)\n",
        "# # print(L3_2.shape)\n",
        "\n",
        "L3_3 = Conv2D(filters=16, kernel_size=(1, 15), strides=(1, 15), padding=\"same\", activation=\"relu\", input_shape=(1, 120, 20))(inp_conc1)\n",
        "# print(L3_3.shape)\n",
        "\n",
        "#L4\n",
        "L3_concat = models.Model(inputs=inp_conc1, outputs=concatenate([L3_1, L3_2, L3_3], axis=2))\n",
        "model.add(L3_concat)\n",
        "model.add(tf.keras.layers.Dropout(.5))\n",
        "# print(dp.shape)\n",
        "\n",
        "#L5\n",
        "L5_1 = Conv2D(filters=16, kernel_size=(1, 2), strides=(1, 2), padding=\"same\", activation=\"relu\", input_shape=(1, 44, 16))(inp_conc2)\n",
        "# print(L5_1.shape)\n",
        "\n",
        "L5_2 = Conv2D(filters=16, kernel_size=(1, 4), strides=(1, 4), padding=\"same\", activation=\"relu\", input_shape=(1, 44, 16))(inp_conc2)\n",
        "# print(L5_2.shape)\n",
        "\n",
        "L5_3 = Conv2D(filters=16, kernel_size=(1, 11), strides=(1, 11), padding=\"same\", activation=\"relu\", input_shape=(1, 44, 16))(inp_conc2)\n",
        "# print(L5_3.shape)\n",
        "\n",
        "#L6\n",
        "L5_concat = models.Model(inputs=inp_conc2, outputs=concatenate([L5_1, L5_2, L5_3], axis=2))\n",
        "model.add(L5_concat)\n",
        "model.add(tf.keras.layers.Dropout(.5))\n",
        "# print(dp2.shape)\n",
        "\n",
        "#L7\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=2, padding='same'))\n",
        "# print(mxpool.shape)\n",
        "\n",
        "#L8 \n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dense(2, activation='relu'))\n",
        "# print(dense.shape)\n",
        "# print(dense2.shape)\n",
        "\n",
        "#L9\n",
        "model.add(layers.Softmax(axis=1))\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_86 (Conv2D)           (None, 1, 120, 20)        180       \n",
            "_________________________________________________________________\n",
            "model_24 (Functional)        (None, 1, 44, 16)         9648      \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 1, 44, 16)         0         \n",
            "_________________________________________________________________\n",
            "model_25 (Functional)        (None, 1, 37, 16)         4400      \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 1, 37, 16)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 1, 19, 16)         0         \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 304)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 100)               30500     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 2)                 202       \n",
            "_________________________________________________________________\n",
            "softmax_12 (Softmax)         (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 44,930\n",
            "Trainable params: 44,930\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyeaU6p3O-wb"
      },
      "source": [
        "60 dim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO-n4x0ORca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fab5c54-bfc6-4b1a-bb07-dfc71f08e9f0"
      },
      "source": [
        "model = models.Sequential()\n",
        "inp_conc1 = Input(shape=(1, 60, 20))\n",
        "inp_conc2 = Input(shape=(1, 22, 16))\n",
        "\n",
        "#l2\n",
        "model.add(Conv2D(20, (8, 1), activation=\"relu\", strides=(8, 1), padding='same', input_shape=(8, 60, 1))) #just change kernel size here and you should be good to go with any array\n",
        "# print(L2.shape) #0th index: # of batches, 1st index: height of output, 2nd index: width of output, 3rd index: depth -> # of filters\n",
        "#l3\n",
        "L3_1 = Conv2D(filters=16, kernel_size=(1, 5), strides=(1, 5), padding=\"same\", activation=\"relu\", input_shape=(1, 60, 20))(inp_conc1)\n",
        "# print(L3_1.shape)\n",
        "\n",
        "L3_2 = Conv2D(filters=16, kernel_size=(1, 10), strides=(1, 10), padding=\"same\", activation=\"relu\", input_shape=(1, 60, 20))(inp_conc1)\n",
        "# # print(L3_2.shape)\n",
        "\n",
        "L3_3 = Conv2D(filters=16, kernel_size=(1, 15), strides=(1, 15), padding=\"same\", activation=\"relu\", input_shape=(1, 60, 20))(inp_conc1)\n",
        "# print(L3_3.shape)\n",
        "\n",
        "#L4\n",
        "L3_concat = models.Model(inputs=inp_conc1, outputs=concatenate([L3_1, L3_2, L3_3], axis=2))\n",
        "model.add(L3_concat)\n",
        "model.add(tf.keras.layers.Dropout(.5))\n",
        "# print(dp.shape)\n",
        "\n",
        "#L5\n",
        "L5_1 = Conv2D(filters=16, kernel_size=(1, 2), strides=(1, 2), padding=\"same\", activation=\"relu\", input_shape=(1, 44, 16))(inp_conc2)\n",
        "# print(L5_1.shape)\n",
        "\n",
        "L5_2 = Conv2D(filters=16, kernel_size=(1, 4), strides=(1, 4), padding=\"same\", activation=\"relu\", input_shape=(1, 44, 16))(inp_conc2)\n",
        "# print(L5_2.shape)\n",
        "\n",
        "L5_3 = Conv2D(filters=16, kernel_size=(1, 11), strides=(1, 11), padding=\"same\", activation=\"relu\", input_shape=(1, 44, 16))(inp_conc2)\n",
        "# print(L5_3.shape)\n",
        "\n",
        "#L6\n",
        "L5_concat = models.Model(inputs=inp_conc2, outputs=concatenate([L5_1, L5_2, L5_3], axis=2))\n",
        "model.add(L5_concat)\n",
        "model.add(tf.keras.layers.Dropout(.5))\n",
        "# print(dp2.shape)\n",
        "\n",
        "#L7\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=2, padding='same'))\n",
        "# print(mxpool.shape)\n",
        "\n",
        "#L8 \n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dense(2, activation='relu'))\n",
        "# print(dense.shape)\n",
        "# print(dense2.shape)\n",
        "\n",
        "#L9\n",
        "model.add(layers.Softmax(axis=1))\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_42 (Conv2D)           (None, 1, 60, 20)         180       \n",
            "_________________________________________________________________\n",
            "model_12 (Functional)        (None, 1, 22, 16)         9648      \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 1, 22, 16)         0         \n",
            "_________________________________________________________________\n",
            "model_13 (Functional)        (None, 1, 19, 16)         4400      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 1, 19, 16)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 1, 10, 16)         0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 100)               16100     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 30,530\n",
            "Trainable params: 30,530\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHUyRZWmRhto"
      },
      "source": [
        "Dynamic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK4tWVTPN4kE",
        "outputId": "7388175b-2158-43f4-d899-e1525042d2bb"
      },
      "source": [
        "model = models.Sequential()\n",
        "dim = PCA_val.shape[2]  \n",
        "inp_conc1 = Input(shape=(1, dim, 20))\n",
        "inp_conc2 = Input(shape=(1, int(dim/5+dim/10+dim/15), 16))\n",
        "\n",
        "#l2\n",
        "model.add(Conv2D(20, (8, 1), activation=\"relu\", strides=(8, 1), padding='same', input_shape=(8, 60, 1))) #just change kernel size here and you should be good to go with any array\n",
        "# print(L2.shape) #0th index: # of batches, 1st index: height of output, 2nd index: width of output, 3rd index: depth -> # of filters\n",
        "#l3\n",
        "L3_1 = Conv2D(filters=16, kernel_size=(1, 5), strides=(1, 5), padding=\"same\", activation=\"relu\")(inp_conc1)\n",
        "# print(L3_1.shape)\n",
        "\n",
        "L3_2 = Conv2D(filters=16, kernel_size=(1, 10), strides=(1, 10), padding=\"same\", activation=\"relu\")(inp_conc1)\n",
        "# # print(L3_2.shape)\n",
        "\n",
        "L3_3 = Conv2D(filters=16, kernel_size=(1, 15), strides=(1, 15), padding=\"same\", activation=\"relu\")(inp_conc1)\n",
        "# print(L3_3.shape)\n",
        "\n",
        "#L4\n",
        "L3_concat = models.Model(inputs=inp_conc1, outputs=concatenate([L3_1, L3_2, L3_3], axis=2))\n",
        "model.add(L3_concat)\n",
        "model.add(tf.keras.layers.Dropout(.5))\n",
        "# print(dp.shape)\n",
        "\n",
        "#L5\n",
        "L5_1 = Conv2D(filters=16, kernel_size=(1, 2), strides=(1, 2), padding=\"same\", activation=\"relu\")(inp_conc2)\n",
        "# print(L5_1.shape)\n",
        "\n",
        "L5_2 = Conv2D(filters=16, kernel_size=(1, 4), strides=(1, 4), padding=\"same\", activation=\"relu\")(inp_conc2)\n",
        "# print(L5_2.shape)\n",
        "\n",
        "L5_3 = Conv2D(filters=16, kernel_size=(1, 11), strides=(1, 11), padding=\"same\", activation=\"relu\")(inp_conc2)\n",
        "# print(L5_3.shape)\n",
        "\n",
        "#L6\n",
        "L5_concat = models.Model(inputs=inp_conc2, outputs=concatenate([L5_1, L5_2, L5_3], axis=2))\n",
        "model.add(L5_concat)\n",
        "model.add(tf.keras.layers.Dropout(.5))\n",
        "# print(dp2.shape)\n",
        "\n",
        "#L7\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=2, padding='same'))\n",
        "# print(mxpool.shape)\n",
        "\n",
        "#L8 \n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dense(2, activation=tf.keras.activations.softmax)) #fix this, figure out softmax dense layer\n",
        "# print(dense.shape)\n",
        "# print(dense2.shape)\n",
        "\n",
        "# #L9\n",
        "# model.add(layers.Softmax(axis=1))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 1, 60, 20)         180       \n",
            "_________________________________________________________________\n",
            "model (Functional)           (None, 1, 22, 16)         9648      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1, 22, 16)         0         \n",
            "_________________________________________________________________\n",
            "model_1 (Functional)         (None, 1, 19, 16)         4400      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 19, 16)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 1, 10, 16)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               16100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 30,530\n",
            "Trainable params: 30,530\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb-A3jje0tOy"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayKcgeXO0siD",
        "outputId": "850e7b03-f51c-41df-aa89-137664155a26"
      },
      "source": [
        "history = model.fit(PCA_train, T_labels, epochs=30, \n",
        "                    validation_data=(PCA_val, V_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "176/176 [==============================] - 2s 7ms/step - loss: 0.7065 - accuracy: 0.5143 - val_loss: 0.6586 - val_accuracy: 0.7429\n",
            "Epoch 2/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.6850 - accuracy: 0.5504 - val_loss: 0.6306 - val_accuracy: 0.7726\n",
            "Epoch 3/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.6704 - accuracy: 0.5818 - val_loss: 0.6671 - val_accuracy: 0.5702\n",
            "Epoch 4/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.6410 - accuracy: 0.6200 - val_loss: 0.6277 - val_accuracy: 0.6667\n",
            "Epoch 5/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.6127 - accuracy: 0.6563 - val_loss: 0.6004 - val_accuracy: 0.6964\n",
            "Epoch 6/30\n",
            "176/176 [==============================] - 1s 7ms/step - loss: 0.5841 - accuracy: 0.6868 - val_loss: 0.5940 - val_accuracy: 0.7036\n",
            "Epoch 7/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.5535 - accuracy: 0.7071 - val_loss: 0.6512 - val_accuracy: 0.5988\n",
            "Epoch 8/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.5235 - accuracy: 0.7328 - val_loss: 0.5937 - val_accuracy: 0.7000\n",
            "Epoch 9/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.4943 - accuracy: 0.7488 - val_loss: 0.6786 - val_accuracy: 0.6298\n",
            "Epoch 10/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.4659 - accuracy: 0.7640 - val_loss: 0.6404 - val_accuracy: 0.7000\n",
            "Epoch 11/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.4477 - accuracy: 0.7856 - val_loss: 0.6487 - val_accuracy: 0.6845\n",
            "Epoch 12/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.4180 - accuracy: 0.8034 - val_loss: 0.7003 - val_accuracy: 0.7155\n",
            "Epoch 13/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.4068 - accuracy: 0.8059 - val_loss: 0.7290 - val_accuracy: 0.6560\n",
            "Epoch 14/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3969 - accuracy: 0.8118 - val_loss: 0.7339 - val_accuracy: 0.6893\n",
            "Epoch 15/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3892 - accuracy: 0.8178 - val_loss: 0.7480 - val_accuracy: 0.6940\n",
            "Epoch 16/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3752 - accuracy: 0.8234 - val_loss: 0.7930 - val_accuracy: 0.7286\n",
            "Epoch 17/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3555 - accuracy: 0.8353 - val_loss: 0.8255 - val_accuracy: 0.6881\n",
            "Epoch 18/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3450 - accuracy: 0.8349 - val_loss: 0.8255 - val_accuracy: 0.6929\n",
            "Epoch 19/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3357 - accuracy: 0.8444 - val_loss: 0.8497 - val_accuracy: 0.6893\n",
            "Epoch 20/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3401 - accuracy: 0.8383 - val_loss: 0.8601 - val_accuracy: 0.6988\n",
            "Epoch 21/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3296 - accuracy: 0.8419 - val_loss: 0.8681 - val_accuracy: 0.6762\n",
            "Epoch 22/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3162 - accuracy: 0.8483 - val_loss: 0.9598 - val_accuracy: 0.7083\n",
            "Epoch 23/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3158 - accuracy: 0.8496 - val_loss: 0.9080 - val_accuracy: 0.6833\n",
            "Epoch 24/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3144 - accuracy: 0.8563 - val_loss: 0.9648 - val_accuracy: 0.7024\n",
            "Epoch 25/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.3025 - accuracy: 0.8524 - val_loss: 0.9643 - val_accuracy: 0.7119\n",
            "Epoch 26/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2935 - accuracy: 0.8684 - val_loss: 0.9695 - val_accuracy: 0.6964\n",
            "Epoch 27/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2909 - accuracy: 0.8658 - val_loss: 0.9602 - val_accuracy: 0.6869\n",
            "Epoch 28/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2756 - accuracy: 0.8718 - val_loss: 1.0091 - val_accuracy: 0.7238\n",
            "Epoch 29/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2873 - accuracy: 0.8663 - val_loss: 0.9726 - val_accuracy: 0.7107\n",
            "Epoch 30/30\n",
            "176/176 [==============================] - 1s 6ms/step - loss: 0.2795 - accuracy: 0.8717 - val_loss: 1.1027 - val_accuracy: 0.7143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "E7hPM0Qs8bTn",
        "outputId": "2485e618-bc7f-41b3-8c9a-b61c46e54beb"
      },
      "source": [
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(PCA_val,  V_labels, verbose=2)\n",
        "y_pred = np.argmax(model.predict(PCA_val), axis=-1)\n",
        "print(y_pred.shape) #TRY ARGMAX\n",
        "\n",
        "# plt.plot(y_pred[:, 0], label='accuracy')\n",
        "# plt.plot(y_pred[:, 1], label = 'val_accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Pred')\n",
        "# plt.ylim([0, 1])\n",
        "# plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27/27 - 0s - loss: 1.1027 - accuracy: 0.7143\n",
            "(840,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEKCAYAAADw2zkCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUVfrA8e+bAgkJpAGhJKEjEEKNgGBBkLWDiggqrqLYu7trYXd/sq7uuquuq2tbVBRsKGDXtYKiUhN6kZ5GDQkJBNLn/P44kxBCykySyaS8n+fJk8ydO3fOzST3vae9R4wxKKWUat58vF0ApZRS3qfBQCmllAYDpZRSGgyUUkqhwUAppRQaDJRSSuHBYCAis0XkoIhsrOR5EZHnRWSHiKwXkSGeKotSSqmqebJm8CZwQRXPXwj0cn7dArzswbIopZSqgseCgTFmCZBZxS4TgLnGWg6EikhHT5VHKaVU5fy8+N6dgdQyj9Oc2/aV31FEbsHWHggKChrap0+feimgUko1FYmJiYeMMe0qe96bwcBlxphZwCyA+Ph4k5CQ4OUSKaVU4yIiyVU9783RRHuA6DKPo5zblFJK1TNvBoNPgd86RxWNALKNMac0ESmllPI8jzUTich7wGigrYikAY8C/gDGmFeAL4GLgB3AcWCap8qilFKqah4LBsaYq6t53gB3eur9lVJKuU5nICullNJgoJRSSoOBUkopNBgopZRCg4FSSik0GCillEKDgVJKKTQYKKWUQoOBUkopNBgopZRCg4FSSik0GCillEKDgVJKKTQYKKWUQoOBUkopNBgopZRCg4FSSik0GCillEKDgVJKKTQYKKWUQoOBUkopNBgopZRCg4FSSik0GCillEKDgVJKKTQYKKWUQoOBUkopNBgopZRCg4FSSik0GCillEKDgVJKKTQYKKWUQoOBUkopNBgopZRCg4FSSik8HAxE5AIR2SoiO0Tk4Qqe7yIi34vIehH5QUSiPFkepZRSFfNYMBARX+BF4EKgH3C1iPQrt9vTwFxjzADgMeDvniqPUkqpynmyZjAM2GGM2WWMKQDmARPK7dMPWOT8eXEFzyullKoHngwGnYHUMo/TnNvKWgdc4fz5cqC1iESUP5CI3CIiCSKSkJ6e7pHCKqVUc+btDuTfA+eIyBrgHGAPUFx+J2PMLGNMvDEmvl27dvVdRqWUavL8PHjsPUB0mcdRzm2ljDF7cdYMRCQYmGiMyfJgmZRSSlXAkzWDVUAvEekmIi2AKcCnZXcQkbYiUlKGR4DZHiyPUkqpSngsGBhjioC7gK+BLcAHxphNIvKYiIx37jYa2Coi24BI4AlPlUcppVTlxBjj7TK4JT4+3iQkJHi7GEop1aiISKIxJr6y573dgayUUqoB0GCglFJKg4FSSikNBkoppdBgoJRSCs9OOlNKqWbDGENK5nGW78pgTUoWnUIDie8SxqCYUFq1qNml1hjDnqxcVqdksTr5MOMHdWJITFgdl9zSYKCUatYOHMlDgLbBLfHxEZdfZ4whNTOX5bsyWL4rg2W7MtiXnQdA6wA/juYVAeDrI/Tt2Jr4LuEM7RLG0C5hdAoNrPCYeYXFbNiTzerkw6xOOcyalCwOHs0HIMDfh34d22gwUEqpupRbUMzT32xl9i+7MQb8fITINgF0DAmgY2ig/e786hASSKeQAPKLHKUX/hW7MtmTlQtARFALRnSPYET3cM7oEUGPdsEcyS1idephEpMOk5h8mPdXpfLm0iQAOoUEMLRrOENjQgkLasGalCxWpxxm894jFDns3K8uEa0Y2SOCIV3CGBITxmkdWuPv67mWfZ10ppRqdlYlZfLggvXsPnSMa4bH0LdDa/Zl57E/O4+92bnsz85jX3Ye+UWOCl8fHtSCEd3DnQEggl7tgxGpulZRWOxgy74jJCYfJiHZBon9R2xNItDfl4HRIQyOsRf+wTGhtA1uWafnXN2kM60ZKKWajdyCYp76eitvLN1N59BA3r15OCN7tK1wX2MMh48Xsi87l31Zeew7kgfGMKybvfi706QE4O/rw4CoUAZEhTJtVDcA9mTlciS3kF7tg/Hz4F2/KzQYKKWahZW7M3lwwTqSMo7z2zO68NAFfQhqWfklUEQID2pBeFALYjuFeKRMnUMD6VxJ/0F902CglGrSjhcU8dTXW3lzaRJRYYG8d/MIzuhxyhpazZ4GA6VUg5adW8gvOw7x49Z0VuzOICTQn57tW9OzfTC92gfTs30w0eGt8K2g2WbFrgweXLie5IzjXH9GFx6spjbQnOlvRSnVoDgchg17slmyLZ0ft6WzJjWLYoehdYAfI7pHkFtQzM870lm4Oq30NS38fOjeNoheka3p2S6YXpHBrNydyZtLk4gJb8W8W0YworvWBqqiwUAp5XXpR/P5abu9+P+0/RCZxwoQgQGdQ7hzdA/O7t2OQdGhJ3WyHskrZMfBnNKv7QeOsjb1MJ+t21u6zw0ju/LgBafVeNJXc6K/IaVUnUvJOM7y3RkcysnnWH4Rx/KLOZpXZH8uKCInv6h0e05+Edm5hQC0DW7B6N7tOOe0dpzZsy0RVQyvbBPgzxDnUMyyjhcUsSv9GC39fOgV2dqj59mUaDBQStVa5rEClu48xC87DvHzjkOkZuaWPucjENzSj+CWfgQ5v1oH+BHZOoCgln4Et/QlMiSAs3u1o1/HNm4P2SyvVQs/+nf2zOifpkyDgVJN0J6sXOYnpLL70DGGdglzeWKUq3ILilmVlFl68d+87wjGQOuWfozoEcH0M7szskcEUWGtCPD3qbP3VZ6jwUCpJqKgyMH3Ww4wb1UqS7anAzbfzidrbRt6RSkTqrtIG2NIz8lnd/oxdh+yXxv2ZJOQdJiCYgf+vsKQmDAeOK83o3q1ZUDnEK9PnlI1o8FAqUZuZ3oOH6xKZeHqNA7lFNChTQB3n9uTSfHRRIUFnpJM7YsN+wDbPj+8ewRndI/g9K7h5BUWs/vQMXYdKrnw57A7/RjHCopL36uFnw892wVz/cgujOrZlmHdwrVztonQT1GpRii3oJj/bdzHvJWprEzKxM9HGNu3PVNOj+Hs3u1OGnMfE9GKmIhWXHV69ElplpfvymTZzgy+WL/vpGP7CESFtaJb2yDiu4TTrW1Q6Ven0MAKx/Orxk+DgVJelp1byJu/JPHLzkP4CPj5+ODrI6VffmW++/gIRcWGxVsPcjSviK4RrXjogj5MHNqZ9q0Dqn0vEaFLRBBdIoKYfHoMxhiSM46TmHyY1gF+dG8XRHR4K1r6+dbDmauGRIOBUl6SdbyA2T/v5o1fkjiaX8TgmFD8fX3ILSym2GEodhiKHIZih4Mih8HhfGwMnNc3ksmnRzO8W3itOmdFhK5tg+jaNqgOz0w1RhoMlKpnh48V8NrPu5izNJmc/CIu7N+Bu8f0ol+nNt4ummrGNBgoVU8ycvJ59afdvLUsieOFxVwU15F7xvTitA46MUp5nwYDpTzsUE4+ry7ZxVvLk8ktLObSAZ24a0xPeuvsWNWAaDBQygMO5eSTmHyYX3Yc4oOEVAqKHIwf2Im7xvSiZ/tgbxdPqVNoMFCqlhwOw470HBKca90mJmeSlHEcgBa+PlwyoCN3jelJ93YaBFTDpcFAKTflFRazJiWLxORMEpIPszr5MEfyigC7Nu7QLmFMGRZDfJcw+ncOIcBfh2mqhk+DgVLVyCssZnXKYZbvtBO11qZmUVBsF0rv1T6Yiwd0ZEhMGPFdw+ka0Urz8KhGSYOBUuWU3Pkvc6ZwWJtiL/4+AnGdQ5g2qivDu4czNCackFb+3i6uUnVCg4Fq9nILilmTepgVuzJZviuDNalZFBTZi3//ziHcMKorI7qHc3rXcFoH6MVfNU0aDFSzk5NfRGLyYVbsymDl7kzWpWVRWGwQgdhObfjtiC6c0SOC+K7hhATqxV81DxoMVJOXfbyQlUmZrNydwYrdmWzae4Rih8HXR4jrHMKNZ3ZjeLdwhnbRi79qvjwaDETkAuA5wBd4zRjzZLnnY4A5QKhzn4eNMV96skyq6TDGkHW8kP1H8jhQ+pV/0s/7j+SRfjQfsOmXB0WHcsfoHgzvFsHgmFCCWur9kFLgwWAgIr7Ai8A4IA1YJSKfGmM2l9ntT8AHxpiXRaQf8CXQ1VNlUk3D5r1HeOTD9WzZf5SCIscpz4cHtSCyTQCRbVoS26kN0eGtiO8SxsDoUB3mqVQlPHlbNAzYYYzZBSAi84AJQNlgYICS7FwhwF4Plkc1csYY3l6ezF+/2EJYK39uGNmVyDYBdGgTQIeQlrRvHUD7Ni01/bJSNeDJYNAZSC3zOA0YXm6fmcA3InI3EAScV9GBROQW4BaAmJiYOi+oaviyjxfy0ML1fLVpP6NPa8czkwYSEdzS28VSqsnw9mKlVwNvGmOigIuAt0TklDIZY2YZY+KNMfHt2rWr90Iq70pMPsxFz//Ed1sOMOOiPsy+/nQNBErVsWqDgYhcWtEF2gV7gOgyj6Oc28q6CfgAwBizDAgA2tbgvVQT5HAYXvlxJ1f9dxkiMP+2M7jl7B746LKLStU5Vy7yk4HtIvJPEenjxrFXAb1EpJuItACmAJ+W2ycFGAsgIn2xwSDdjfdQTdShnHxueHMVT/7vV86PjeSLe85icEyYt4ulVJNVbZ+BMWaqiLTB2aQjIgZ4A3jPGHO0itcVichdwNfYYaOzjTGbROQxIMEY8ynwO+BVEbkf25l8gzHG1P60VGO2dMch7n1/Ldm5hTx+WX+uHR6j+X6U8jBx9dorIhHAdcB9wBagJ/C8MeY/niveqeLj401CQkJ9vqWqJ0XFDp7/fjv/WbyD7m2DeOGaIfTtqEtBKlUXRCTRGBNf2fPV1gxEZDwwDXvxnwsMM8YcFJFW2GGi9RoMVNN0LL+IO99dzQ9b07lyaBSPTYilVQudEKZUfXHlv20i8KwxZknZjcaY4yJyk2eKpZqTQzn53PjmKjbuyeaJy/tz7fAu3i6SUs2OK8FgJrCv5IGIBAKRxpgkY8z3niqYah6SM47x29krOXAkj1nXxXNev0hvF0mpZsmV0UTzgbJz/oud25SqlfVpWVzx0lKO5BbyzvQRGgiU8iJXagZ+xpiCkgfGmALnUFGlauyHrQe5453VhLVqwdybhtFD1wdWyqtcqRmkOzuRARCRCcAhzxWpAdiwADJ3e7sUTdaCxDSmz0mga0QQH90xUgOBUg2AK8HgNmCGiKSISCrwEHCrZ4vlRYlzYOFNsPxlb5ekyTHG8OLiHfx+/jqGdw/n/VtH0L5NgLeLpZTCtUlnO4ERIhLsfJzj8VJ5S8oK+OJ39uesFO+WpYkpdhhmfrqJt5YnM2FQJ566ciAt/LydGkspVcKlgdwicjEQCwSUzAQ1xjzmwXLVv+w98P5UCI2G4EjITvN2iZqMvMJi7pu3lq827eeWs7vz8AV9NL+QUg2MK4nqXsHmJ7obEGAS0LQGghfmwvvXQuFxmPIuRMZCttYM6sLuQ8e49rUVfL15P3++pB8zLuqrgUCpBsiVmsFIY8wAEVlvjPmLiDwD/M/TBas3xsBn98HeNTYQtO8LIdGQlw15RyBA0yHUREGRg1lLdvL8oh209PPhhauHcPGAjt4ullKqEq4Egzzn9+Mi0gnIAJrOf/Xyl2D9PBg9A/pcbLeFRNnv2WkQ0M97ZWukViVlMuPDDWw/mMPFcR159NJ+2lGsVAPnSjD4TERCgaeA1djsoq96tFT1Zeci+OZP0PdSOPsPJ7aHOldTy06DSA0Grso+XsiTX/3KeytT6BwayOwb4hnTRyeSKdUYVBkMnIvafG+MyQIWisjnQIAxJrteSudJmbtg/jRo1wcuewV8ynSflNYMtN/AFcYYPlu/j8c+20zmsXxuPqsb94/rrYnmlGpEqvxvNcY4RORFYLDzcT6QXx8F86j8o/DeNSBi+wlalpv0FNwBfPx1RJELUjOP86ePN/LjtnQGRIXw5rTT6d85xNvFUkq5yZVbt+9FZCLwYZNYeMbhgI9ug0Pb4LoPIbzbqfv4+ECbTpCVWv/layQKix28/vNu/v3dNnxF+L9L+nH9yK746kghpRolV4LBrcADQJGI5GGHlxpjTOMcZrPkn/Dr53D+36H76Mr3C42BbA0GFdmy7wi/n7+OTXuPcF7fSB6bEEun0EBvF0spVQuuzEBuXR8FqRdbPoMf/g4Dr4ERt1e9b0gU7F5S9T7NTGGxg5cW7+SFxdsJCfTn5WuHcGFc0xlYplRz5spKZ2dXtL38YjcN3oHNtnmo81C45FnbX1CVkGg4ug+KC8HXv37K2IBt3mtrA5v3HeHSgZ34y/hYwoM0ea1STYUrzURlxlwSAAwDEoExHimRp+xeAi2CYfI74O/CmPeQKDAOOLIXwprWhGt3FBQ5eOmHHbywaAehrVrwytShXNC/g7eLpZSqY640E11a9rGIRAP/9liJPGXEbTBwCgSGurZ/aLT9np3abIPBpr3Z/H7+erbsO8KEQZ2YeWksYVobUKpJqslA8DSgb10XpF64GgjANhNBsxxeWlDk4MXFO3hxsa0N/Pe6oZwfq7UBpZoyV/oM/oOddQw2sd0g7Ezkpq1k4lkzG15atjZw+eDOPHppP0JbaW1AqabOlZpBQpmfi4D3jDG/eKg8DYd/IAS1a1bDS5ftzOCGN1bSJtCfV38bzzhdk1ipZsOVYLAAyDPGFAOIiK+ItDLGHPds0RqAkKhmEwzWpmYxfc4qYsJbMe+WEUQEt/R2kZqGvWuh3Wn25kKpBsyVpaa+B8r+JQcC33mmOA1MSHSz6DPYuv8oN7yxkojglrw9fbgGgrqycxHMOgc+vsPbJVGqWq4Eg4CyS106f27luSI1ICHRts+gCWThqExyxjGmvr6Cln4+vDN9OJGaarpuHM+0QcC3BWz6UCcwNjcHf4WCxtV44kowOCYiQ0oeiMhQINdzRWpAQqOhKNf+YzdB+7Jzufa1FRQVO3j7puFEhzePGO9xxsDn98OxQ3DDFxDaBb78g53AqJq+XT/CS8Ph+cGQOAeKi7xdIpe4EgzuA+aLyE8i8jPwPnCXZ4vVQDThVNYZOflMfW0FWccLmXvjcHpFNp2sI163/n3Y/DGcOwOih8EFT0L6r7Div94umfK0gmPw6d0Q1s3mN/vsHnh5JPz6ZYNvYag2GBhjVgF9gNuB24C+xphETxesQWiicw2O5BVy/RsrSTucy+vXxxMXpSmn60xWiq0FxJwBo+612067EHqOgx+ehKP7vVs+5VmLHoesZLjsJbjpG5j8NphimHc1vHEhpK70dgkrVW0wEJE7gSBjzEZjzEYgWESaR49YSTBoQnMNcguKuenNVfy67yivTB3K8O4R3i6S+4qL4Md/2nbZhsRRbPNfGQOXvwI+vna7CFz4DyjOh2//z7tlVJ6TsgKWvwyn3wxdRtrPve+lcMdyuPhfkLETXh8H866FQ9vdO7ajGA4nebTJ2pVmopudK50BYIw5DNzssRI1JK3Cwb9Vk6kZFBQ5uO3tRBKSD/PvKYM4t097bxepZhb9FRY/Ad/80dslOdnS/0DyL/bCH9b15OciesDIe2wTUvJSrxSvWXE47AW0vhTmwad32RvI8x49+Tlffzj9JrhnDZz7R9j1A7w4HD677+SaojGQc9D+fayea28c5l1r932iAzw3EDZ/4rFTcGWega+ISMnCNiLiCzSPKakizrkGjb/PoNhhuO/9Nfy4LZ0nr4jjkgGdvF2kmtnyGfzyb2jdEXZ8D5m7K16gqL7tW2+bCPpeCoOuqXifsx6AdfNsM9ItP4KvLgta57JSYc3b9iv/KIz7Cwy5/uRlbT3hx3/YBbOmfggtK+l/axkM5zwIQ6fZdVUSZtubg55j7Q1nxk7IP3Jif98WEN4dwntAr99ARE/oeqbHTsGVv8avgPdFpKT361bgfx4rUUPTBOYaOByGRz5cz5cb9vOni/syZViMt4tUM4d2wEe3Q6chcOXr8J+hsHoOnDfTu+UqzIMPb7E1yUueqzw9eosguOBv8MFvIeF1GH5r/ZazPjkckDgbeoyxFzRPKi6EbV/ZkTs7nFOgeoyB4gL4/D7YMB8ufQ7a9vLM++9dC788B4Om2gt7dYLbwUVPwfDbbA13T6L9HQ2cYi/4ET3s95DoE02N9cCVYPAQcAu28xhgPdB8spaFRMG+dd4uRY0VFDmY8dEGFiSmcc/YXkw/y8P/mJ5ScAzen2qr3FfNtcN+e19o7wBHzwA/L1ZWv/8LpG+BaxdCUDV9MH3HQ/dzYdETEHuFvTA0Rb88C98/Zmtw0770TEDI3GWbU9a+CzkHoHUnOPsPMHiqzTRsjP37+OaP8PIoe1c+6t66XZ+kuBA+ucumrjn/cfdeG9EDrpxdd2WpJVdGEzmAFUASdi2DMcAWVw4uIheIyFYR2SEiD1fw/LMistb5tU1Esio6jleFRsPxQw1vAknGTkh8E7L3VLpL5rECpr6+ggWJadw7thf3n+ehOyNPMwY+vQcObbX/PCXpxeNvhGPpdhlTb9m5GJa/ZDsNe51X/f4icOE/ofA4fDfT48XziuSltsmsxxgoyoM54+tuEEZRPmxYAHMuteP4f3neLlh19ftw3wYY88cTKedFYMh1cOcqO6Jr0V9h1mh7J15Xfv43HNgAl/wLAsPq7rheUGnNQER6A1c7vw5h5xdgjDnXlQM7+xZeBMZh016vEpFPjTGbS/YxxtxfZv+7gcE1OAfPKhlRdGSP56qZriguhJTltjq87WvIcI5GiL/RrtxWzo6DOdw0ZxX7svN4bsogJgzqXM8FrkMr/gsbF8CYP0OPMn9+PcbYsdwJs6H/FfVfrpJZxm17w7jHXH9du95wxh22aWHoDRB9useKWO+OHYIFN9px9pPm2Lv3OePtxXva/6BNLZZJ3b0EFk63tYDQGBjzJxh0LbSppv+rdSRcNQd+/QK++B28dh6MuMPOA2kRVPPyHNxi2/77T4Q+F9f8OA1EVTWDX7G1gEuMMWcaY/4DuNM9PwzYYYzZZYwpAOYBE6rY/2rgPTeOXz9Kh5d6oRP5WIbtcJx/A/yzB8y5BFbOsnfGF/4TokfY4Wzl/Lz9EJe/9AvH8ot47+YRrgcChwPevMTeedWV4iJ7zM8fgLxs91+fstxW80+7CM584OTnfHzsxTTpJ/eH6tWWMfbCcuwgXDELWrg5e/vsB20Type/q99RLxXJz4FNH9sLeW04HLbv5HgmTHoTAtpAp0EwdaGtwc0dDznp7h/XGFj6Asy9DAJCbCftPetsk1B1gaCsPhfDnSvs38yyF+ClEXYQQk04im3zUMvW9n+xCagqGFwB7AMWi8irIjIWqGbh4JN0BsrWDdOc204hIl2AbsCiSp6/RUQSRCQhPb0Gf0y1UToLuZ46kXMPw5Kn4bVx8FQP+OhWW+3uN94u2fngbrjuI9v52HMsHNwMuSda195ensz1b6ykU0ggH985iqFd3Ki6pm+xF9blL9fd+ez+0R4z4XV4cYSdiemqowfgg+vtXeBlL1c8ImTwdeDjZ5vM6tOG+Tbn0OiHoVMNKrQtg+E3j9v+qPouO9gL7J7V8Nm98MxpMP96eP03tbvp+flfsPN7uPBJ6DjgxPbo0+GaD2xT0VuXuTdWPj/H1jS++SP0uQhuXmT/7ms6OiggxNakp/3PjtZ5+wo7N8TdZqzlL8OeBBsIgtrWrCwNTKW/UWPMx8aYKdjZx4uxaSnai8jLIvKbOi7HFGBBSZrsCsoyyxgTb4yJb9eunjvc2nQC8amfVNZF+fDuZNu2WVwA5zwENy+GB36FCS9A30vsRaRE9HDAQFoCRcUOZn66iT99vJFzerdj4R0jiQpz8261ZPz7ngQ7waUubPwQWobYf77AMDsT84Pr7YW+KsWFsGCarU1c9Vblq9QFt7fDOde+A4X1kDKrMBeSfoEvfm9//6Pur/41lek/EbqeZT/v+sp/lZcNK1+F/54Fr54L6963ndqXvWz7xmZfWLNaVtIvdmRM/4l26GR5XUfB1e/aY791uWu1xJJJWps/tiPGrnqr8mGb7uoyEm77Bc76vQ3s/46Dt6+0Q5eryyGVsdP2iZx2kT3fJsKVNZCPAe8C74pIGDAJO8Lom2peugeILvM4yrmtIlOAO6strTf4+ttRCp6uGRgDX/4eUlfAlW+41gYeFQ/iS/7updz6cxt+2JrOTWd2Y8ZFffH1cacS55SyzF6487PtRfysB6p/TVWK8u0/V99L7D/fLT/A0ufs7OFdi+E3T9iRHxUNxfxupp3Adfks6NC/6veJvxE2fWQn5AycUrsyl1VwDPZvtHfv+9ba7we32PQCLdvA5f+t3VyBks7kV860I5Iufa7i/Yry4cCmE+U4sMne4Ub0PHkoYpuoiu+YjbF/V4lz7O+pKBc6xMFFT0PcpBOBtkOcvVDPvgCu+xA6DnTtPHLSYeFNtp/gkn9XPrS2xxg7Euz9a+2F97oqxuRv/co2Ofn42GamHmNcK4s7/ANg7J9h6PWw+i078uj9qRAcaeeKDPntqaOgHA47mMG3hZ1VXNm5NkJiPJQ8SUT8gG3AWGwQWAVcY4zZVG6/Pti5DN2MC4WJj483CQkJ1e1Wt14/3zZFTPvCc++x8lUbDM76HYx1PWVBwUtnsTnDcGXuDB6b0J9rhtdwDoEx8K9+EDPCBr7CXLj955odq8SvX9qawNSF0LPMSJv0bbZ5ImUpdDvbXgTL/tNt+sj2k5x+M1z8tGtlfyEeWkXYfDA1dWCTzThZctE9tA2Mwz7Xqq1t/+44yF4kY86ou2GhX82wI5JuXgTt+9py7F1zohwHt4DDmfkyIAQi4+zkpIydUHjsxHF8W9rfY0lwiOhp78BXz7UjsVq0hrgr7UWu0+CKL2SHdsDcCfb413wAXc6ouuwOB7wz0dYMpn93cvNQZTZ/AvOn2d/htfNP7m9xOOwErh+ftL/nq946MTrI04qLYMe39ve17Wsb9LudbSet9b0U/FrCqtfhiwdg/H/s77EREZFEY0x8pc97Khg43/wi4N+ALzDbGPOEiDwGJBhjPnXuMxO7ZsIpQ08r4pVgsHC6TTB133rPHD/pZ/sP2PM8mPKey9NcoGwAAB9uSURBVO2hicmZbHvzTi4z37PmmnWM7F2LkRqHk+x094uetp1jXz0Ed660q3TV1IKbbA3gd1tPHdvtcEDiG/Dto/ZCd+4jMOJOyNwJr46xF8UbvnR9/sDSF2y78u1LITLW/bKmrLCJxEwxBHewF6JOzgt/x0HO5kIP3QXmHbET6IrybG2kpLU0MPzkMnQaZNNhl5TDGJvOIGOH/b1l7LABImOHnZntcDZ3RJ1uL2ixl5/czFiZkrb97D020VpVQ2aXPG2buS75N8RX0DxUmfXz4cOb7eiwKe/Zu/Tcw7Y2sP0bGHiNHa7prRXijuy1TY+r59p+lMBwW4ta+y5EDYXrPm50tQKvBgNP8Eow+G6mzTvzp4N1PyMwK8WOfW4VYe+sAlzLILp57xGuePkXprRKYGb+07ZvofOQ6l9YmbXvwce32XbUoLbwTB/bZ3HuIzU7XsExeKqnbbapYOhrqSN77aicrV/ai17BccjLsukaQtwYDns805Z5yG9dq02UlZsFr5xl/7lv+OLEPIb6tO0bGxwjY09c/EOian7BKS6yaVSMsTUFd+Wkw9uX22SAE1+1gaS8pJ/tkNHYK2Dia+6Xdc3b8MmddvLg6IdtbTA71ab8Pn16w7jYOhyw+wfbxPbrF7Z56I5l9VdbqUPVBQNNjuKKkGh795pzwL2hbNUpOAbzrrH/uFPeczkQZB0v4Na3EwgJ9Oeu66fCrKdtm3BtgkHKUvv+7fvZmknXM2HjQvtPWpN/ym1f24lV1XWwtekEU961TQdf/sF2Yv72E/cCAdhUELGX26G458107Q4YTixEc2QP3Pi1dwIBQO/f2K+64utXu1m/we3g+s/tgIYFN9pRPUOuO/F8Trqt+YV1g0ur6CeoyuCptjnyy9/Dtv/ZGtkNX0LM8JqXu675+Nj+ih5j7NDbgpxGGQhc4eHsTU2EJ1JZG2PHKe/faPPstO3p0suKHYZ75q1lf3YeL08dSttO3SAkxo7Hr43kZbYNt6SJqv9EO7Ft/4aaHW/jQjuOPqaaNmewF5LYy+CuVbaZp9vZNXvP+Buh4Kh9b1etedsOET13RtOa/FUXAkPtMObu59qMnMtetNsdDtvEk3vYTuaqzQifYTfDxc/YNvlbf2xYgaC8oLanZqNtQjQYuKLkbrEuh5f+/Ky9CJ33KPQa5/LLnv12G0u2pTNzfCxDYpxzCGKG25pBTZv8ctLthb/shbvveNtp7s6FtUReNmz/1t6pu9OsFhhq+wpqKnqYrdkkuJjv5dB2+N+DdnjnmbUYItqUtWgFV8+DfhPg6xmw+G/w0zO2L+jCf9gRSLV1+nTbN9G6+aQ8a4g0GLiidOJZHQWDbV/bJF79J8Ko+1x+2deb9vPC4h1Mjo/mmrKZR6OHw9F9NZ8wlOKcX9Bl5IltQRH2jnDjh+4HmV+/tAu5xNZziggRWzvYt9ZOqKpKUb6dx+AXYGcQ12N2yEbHr4Ud7jx4qh3ps/hx6H+lncmrmgwNBq5o2RoCQutmrkH6Njs6qUMcjH/B5bbWHQdz+N0H6xgYFcJfJsQiZV8XM8J+Tz01NYVLkpeBX6DttCyr/0TbCZnmZof9xoW26Sqq0r4qzxlwlV2QKPGNqvf77i+2CWzCi3XbD9RU+fjav9czH4AuZ9a8n0A1WBoMXBUSXfs+g9wsO+7et4XtNHUxn83RvEJufSuBln4+vDx1KAH+5e5i2/ezY8hr2m+QstReuMsP4+xzkR277k5T0bEM24TQ/wrvXCwCQuxY+g0LKp/luv1bWP6incfQ56L6LV9jJmKbNad9UXczgVWDocHAVaG1XOTGUWw73Q4nncjH78rLHIbfz19HUsZxXrhmCJ1CKxh37eNrL+Y1qRnkHbF3yBV19AaE2P6MTR+5nkxty6d25JU3p+nH32hHMq3/4NTnjh6wuWja94Pf/LX+y6ZUA6XBwFUhUbXrM1j0uJ1Mc+E/bZ4WF738406+3nSARy7swxk9qlg4JWaEnbnqbmbQtJV2lm1lM037T4Sc/a6v27txIUT0qpuOxZrqNNg2eSXMPrm/w+GwcykKcuy6CN6a0KRUA6TBwFUh0XaKfk3SMOdl29z1A6+2C2O76Mdt6Tz9zVbGD+zETWdWs85vadK6Ve6VLXkZiC9EDav4+d7ng3+Qa01FR/fbiUj9J3q/PTn+RpvRNXXliW3LX4Sdi+D8v9Vu1JJSTZAGA1eVjCiqSb9BynKbYqCyhdIreknGce55bw2nRbbmyYlxJ3cYVyQq3mZXrWB9g6rfaJnNJ1PZJK0WQXaVqM2fVJ/NcdPHgPHOQjPl9Z9ok8mVDDPdu8Z2Gve5xAYKpdRJNBi4KtQ5lLMmTUVJP9lO4yjXJjXlFhRz69uJGGP473VDadXChYniLVtDZH9IdaMTuSjfjhSKGVn1fv0nQm6mTeJWlY0LbRK12uQzqistg2HAZNvfkZViZ9EGt7cJxrxda1GqAdJg4KraLHKT9LMNBC60URtjePjD9fy6/wjPXz2YLhFuLMsXMwLSEm16C1fsXWPnA1SXmbLnWJvaetOHle9zONn2PzSEWkGJ+Gn2/F4/33bcXzHLpq1QSp1Cg4Grgtrbu3t3J3blZdtUxF3PdGn3t1ek8MnavfxuXG9Gn9bevfeKHm5TGh9wMYVESadwdSkj/FradAFbPrO1iYps+sh+b0jBIDLWOSFvr13ExMXPQKnmSIOBq3x8oE1n92sGKcvtaB0XLkQpGcf5+5dbOKtXW+4Y7VquopOUTD5ztd8gZZldzN2VZfv6X2E70Hd8V/HzGxdC5/iGl7vl/L/BGXfZDKxKqUppMHBHaLT7fQYu9hc4HIYHF67DR4R/TByAT01WKguJsqtdudJv4Ci2QcOVRHIA3c6xabYrGlV0aDvsX98wlwCMiofzn6jdimRKNQMaDNwREuN+zcDF/oJ3VqawfFcmf7y4b8UTy1wVM9xe5KvLJ3Rws13esks1ncclfP2g32Ww9X829XZZGz8EnJlHlVKNkgYDd4RE2bH0RQWu7e9if0Fq5nGe/HILZ/Zsy5TTa5lPP3qEbSOvrgaTvMx+d7VmAPbOv/A4bPvqxDZjYOMC6DJKc/wo1YhpMHBHaDRg7EIornChv6Bk9BDg2nyC6pTkg6+u3yBlqe0DCXVjzeSYM+waBRvLjCo6sMmuFdyQOo6VUm7TYOAOd1NZJ/1kE71VNrsXeG9lKr/syGDGxX2JCnMtcV2V2sdCi+Cq+w2MObGYjTvBx8fHpqXe/s2JmdgbF9oZzP0m1K7cSimv0mDgjpIVz1ztNyjtLwio8Om0w8d54ovNjOwRcfL6BLXh62c7TauqGRzebfMNVTe/oCL9J0JxgV0P1hgbDLqPdm1EklKqwdJg4I42znV5XUlJUU1/gTGGRz7cgAH+MXFA7ZuHyooeAQc32YykFSmdX+Bi53FZnYdAaBcbBPashqzkhjmKSCnlFg0G7vAPgOBI15qJkpdV2V/w/qpUftp+iEcu6kt0eB00D5UVM9y+d2VJ65KX2cV62vVx/9gi9uK/czGsnGWHzfa5uHblVUp5nQYDd7mayrq0v+DU+QV7snJ5/IstnNE9gmvrqnmorKjTbdK6ytY3SFlqh5T61PDj7z/RJt5bPw96jrNrFyulGjUNBu4KcXGRm0r6C0qah4odpuaTy6rTsrVNxVDRymdHD0DmLveGlJYXGQttncnodBSRUk2CBgN3hUTZYFDVpK7cLDsjt4ImovkJaSzZls7DF/YhJqKOm4fKih5hM5KWT1qX4uwvcHWyWUVEYMh1dkZy7wtqfhylVIOhwcBdoTFQlAfHDlW+TyXzC/Zl5/LXzzczrFs4143o4tlyxoxwJq3bePL25GV2wfiOA2t3/BF3wv2bKl8HQSnVqGgwcFfpXIMqspdW0F9Q0jxU6HDw1JUeah4qK9o5+ax8v0HKUjv01Ne/dsf38dFlI5VqQjQYuMuVuQYV9BcsSEzjh63pPHRBH/fWKKip0Gg7FLZsv0FeNuzfWLMhpUqpJk2DgbuqW/6ygv6Cg0fzbPNQ13CuP6Or58tYInr4yTWD1JWAqdlkM6VUk6bBwF2BYTbdQ2XDSyvoL/jLZ5vJK3Lw94lxnm8eKitmhM2jVBK4kpeCj5/Ly28qpZoPDQbuEjkxoqgi5foLvt9ygC/W7+Puc3vSo109d7aW7zdIWWY7jlvUQzOVUqpR0WBQEyHRlS9/Waa/4Fh+EX/+eCO92gdz6zk96reMAJH9wT/I1lYK82BPYu3mFyilmiwNBjURWsnEs3L9Bc98s4292Xk8OTGOFn5e+FWXJK1LXQ57V9sEc7WZX6CUarI0GNRESBTkZp664leZ/oJ1qVm8uXQ3U0fEMLRLuHfKCbbf4MAm2P6t87HWDJRSp/JoMBCRC0Rkq4jsEJGHK9nnKhHZLCKbRORdT5anzoQ48wmVrx04+wsKOw3l4Q830Da4JQ9eUINkcHUp2pm0LuF1m5iulRcDk1KqwfJYMBARX+BF4EKgH3C1iPQrt08v4BFglDEmFrjPU+WpU5UNL036GaKHMXv5PrbsO8JjE2JpE1DLyV21VZK0Li9bawVKqUp5smYwDNhhjNlljCkA5gHll8O6GXjRGHMYwBhz0IPlqTuhJRPPygQDZ39BVvvhPPvdNsb1i+T82A7eKV9ZAW3s6meg/QVKqUp5Mhh0BsreOqc5t5XVG+gtIr+IyHIRqTDrmYjcIiIJIpKQnp7uoeK6IbiDXeqxbDBIsesXvLi7A74iPDYhtm4XrKmNknWRtWaglKqEXwN4/17AaCAKWCIiccaYrLI7GWNmAbMA4uPjq0gXWk98/Wyqh7J9Bkk/U+zTgrmpbZkxvg8dQxpQ3p4z7rRpp0tqNEopVY4ng8EeoOzVJ8q5raw0YIUxphDYLSLbsMGhkiW6GpCQqJP6DIp2LWGNoxd9o9sz1dMZSd0V3t1+KaVUJTzZTLQK6CUi3USkBTAF+LTcPh9jawWISFtss9EuD5ap7pSda5Cbhc+BDSwt6sPfr4jDtz5TTiilVB3wWDAwxhQBdwFfA1uAD4wxm0TkMREZ79ztayBDRDYDi4E/GGMyPFWmOhUSZfP+FBexZcXX+GBoP+A8+nZs4+2SKaWU2zzaZ2CM+RL4sty2/yvzswEecH41LiHRYIrJy9rD+p8/pzv+XH5p+cFSSjV9hYWFpKWlkZeX5+2iKCAgIICoqCj8/d0b1u7tDuTGy7muwYeLlhFXsJ68jkMICdQEcKr5SUtLo3Xr1nTt2rXhjKBrpowxZGRkkJaWRrdu3dx6raajqCnnyJyt65YT65NMSN8xXi6QUt6Rl5dHRESEBoIGQESIiIioUS1Ng0ENHQuwE8qu8PsFH8wp6x0r1ZxoIGg4avpZaDCoAWMMD3++i0wTzEC22fULOsd7u1hKKVVjGgxqYO6yZD5bt5fi1s4J1dHDTlrvWCmlGhsNBm5anXKYx7/YzNg+7WnbuafdqE1ESjULRUVF3i6Cx+hoIjdkHivgrndWE9kmgH9dNQj50TnBWoOBUgD85bNNbN57pE6P2a9TGx69NLba/S677DJSU1PJy8vj3nvv5ZZbbuGrr75ixowZFBcX07ZtW77//ntycnK4++67SUhIQER49NFHmThxIsHBweTk5ACwYMECPv/8c958801uuOEGAgICWLNmDaNGjWLKlCnce++95OXlERgYyBtvvMFpp51GcXExDz30EF999RU+Pj7cfPPNxMbG8vzzz/Pxxx8D8O233/LSSy/x0Ucf1envqC5oMHBRscNw77w1HMopYOHtIwlp5W+zgO76QfsLlGoAZs+eTXh4OLm5uZx++ulMmDCBm2++mSVLltCtWzcyMzMB+Otf/0pISAgbNmwA4PDhw9UeOy0tjaVLl+Lr68uRI0f46aef8PPz47vvvmPGjBksXLiQWbNmkZSUxNq1a/Hz8yMzM5OwsDDuuOMO0tPTadeuHW+88QY33nijR38PNaXBwEX/WbSdn7Yf4m+XxxEXFWI39ptgv5RSAC7dwXvK888/X3rHnZqayqxZszj77LNLx9uHh9uFnb777jvmzZtX+rqwsLBqjz1p0iR8fX0ByM7O5vrrr2f79u2ICIWFhaXHve222/Dz8zvp/a677jrefvttpk2bxrJly5g7d24dnXHd0mDggh+3pfPc99u5Ykhnrh6mmT+Vamh++OEHvvvuO5YtW0arVq0YPXo0gwYN4tdff3X5GGWHZJYfpx8UdGJC6Z///GfOPfdcPvroI5KSkhg9enSVx502bRqXXnopAQEBTJo0qTRYNDTagVyNPVm53DdvDadFtuaJy+J0PLVSDVB2djZhYWG0atWKX3/9leXLl5OXl8eSJUvYvXs3QGkz0bhx43jxxRdLX1vSTBQZGcmWLVtwOBxVtulnZ2fTubMdSfjmm2+Wbh83bhz//e9/SzuZS96vU6dOdOrUiccff5xp06bV3UnXMQ0GVSgocnDnO6spLDa8dO0QAlv4ertISqkKXHDBBRQVFdG3b18efvhhRowYQbt27Zg1axZXXHEFAwcOZPLkyQD86U9/4vDhw/Tv35+BAweyePFiAJ588kkuueQSRo4cSceOHSt9rwcffJBHHnmEwYMHnzS6aPr06cTExDBgwAAGDhzIu++eWNL92muvJTo6mr59+3roN1B7YnPFNR7x8fEmISGhXt7r0U82MmdZMq9MHcIF/Sv/41CqOduyZUuDvsg1BHfddReDBw/mpptuqpf3q+gzEZFEY0ylo10aZuNVA/Dpur3MWZbM9DO7aSBQStXY0KFDCQoK4plnnvF2UaqkwaAC2w8c5eGF64nvEsZDF/bxdnGUUo1YYmKit4vgEu0zKCevsJg73llNqxa+vHDNEPx99VeklGr6tGZQzrPfbmP7wRzm3jiMDiGab0gp1TzobW8Zq1MO8+pPu7h6WAxn927n7eIopVS90WDglFdYzO/nr6NjSCAzLtJ+AqVU86LNRE7/+nYbu9KP8dZNw2gd4N7aoUop1dhpzQBITD7Ma87mobN6afOQUk1ZcHCwt4vQIDX7mkFeYTF/WKDNQ0rVif89DPs31O0xO8TBhU/W7TEbgKKiogaVp6jZ1wxKmof+MXGANg8p1Qg9/PDDJ+UamjlzJo8//jhjx45lyJAhxMXF8cknn7h0rJycnEpfN3fu3NJUE9dddx0ABw4c4PLLL2fgwIEMHDiQpUuXkpSURP/+/Utf9/TTTzNz5kwARo8ezX333Ud8fDzPPfccn332GcOHD2fw4MGcd955HDhwoLQc06ZNIy4ujgEDBrBw4UJmz57NfffdV3rcV199lfvvv7/Gv7dTGGMa1dfQoUNNXUlIyjRdH/7cPPLh+jo7plLNzebNm736/qtXrzZnn3126eO+ffualJQUk52dbYwxJj093fTo0cM4HA5jjDFBQUGVHquwsLDC123cuNH06tXLpKenG2OMycjIMMYYc9VVV5lnn33WGGNMUVGRycrKMrt37zaxsbGlx3zqqafMo48+aowx5pxzzjG333576XOZmZml5Xr11VfNAw88YIwx5sEHHzT33nvvSfsdPXrUdO/e3RQUFBhjjDnjjDPM+vUVX7sq+kyABFPFtbXh1FHqWUnzUKeQQB7RWcZKNVqDBw/m4MGD7N27l/T0dMLCwujQoQP3338/S5YswcfHhz179nDgwAE6dOhQ5bGMMcyYMeOU1y1atIhJkybRtm1b4MRaBYsWLSpdn8DX15eQkJBqF8spSZgHdtGcyZMns2/fPgoKCkrXXqhszYUxY8bw+eef07dvXwoLC4mLi3Pzt1W5ZhsMSpqH3r5puDYPKdXITZo0iQULFrB//34mT57MO++8Q3p6OomJifj7+9O1a9dT1iioSE1fV5afnx8Oh6P0cVVrI9x999088MADjB8/nh9++KG0Oaky06dP529/+xt9+vSp83TYzbLPIDHZTi67ZngMZ/Zq6+3iKKVqafLkycybN48FCxYwadIksrOzad++Pf7+/ixevJjk5GSXjlPZ68aMGcP8+fPJyMgATqxVMHbsWF5++WUAiouLyc7OJjIykoMHD5KRkUF+fj6ff/55le9XsjbCnDlzSrdXtubC8OHDSU1N5d133+Xqq6929dfjkmYXDPIKi/nDfNs8NOMiTburVFMQGxvL0aNH6dy5Mx07duTaa68lISGBuLg45s6dS58+rjUFV/a62NhY/vjHP3LOOecwcOBAHnjgAQCee+45Fi9eTFxcHEOHDmXz5s34+/vzf//3fwwbNoxx48ZV+d4zZ85k0qRJDB06tLQJCipfcwHgqquuYtSoUS4t1+mOZreewRNfbObVn3bzzvThjOqptQKlakvXM6hfl1xyCffffz9jx46tdJ+arGfQrGoGicmZvPbzbq4ZHqOBQCnVqGRlZdG7d28CAwOrDAQ11Ww6kG3z0HptHlJKsWHDhtK5AiVatmzJihUrvFSi6oWGhrJt2zaPHb/ZBIOXftjJrkPHeGf6cIJbNpvTVqpeGGMQEW8Xw2VxcXGsXbvW28XwiJo2/Tebq+KNo7oSE95Km4eUqmMBAQFkZGQQERHRqAJCU2SMISMjg4AA99diaTbBILRVC64cGuXtYijV5ERFRZGWlkZ6erq3i6KwwTkqyv1rXbMJBkopz/D39y+dOasaL4+OJhKRC0Rkq4jsEJGHK3j+BhFJF5G1zq/pniyPUkqpinmsZiAivsCLwDggDVglIp8aYzaX2/V9Y8xdniqHUkqp6nmyZjAM2GGM2WWMKQDmARM8+H5KKaVqyJN9Bp2B1DKP04DhFew3UUTOBrYB9xtjUsvvICK3ALc4H+aIyNYalqktcKiGr22omto5NbXzgaZ3Tk3tfKDpnVNF59Olqhd4uwP5M+A9Y0y+iNwKzAHGlN/JGDMLmFXbNxORhKqmYzdGTe2cmtr5QNM7p6Z2PtD0zqkm5+PJZqI9QHSZx1HObaWMMRnGmHznw9eAoR4sj1JKqUp4MhisAnqJSDcRaQFMAT4tu4OIdCzzcDywxYPlUUopVQmPNRMZY4pE5C7ga8AXmG2M2SQij2GXX/sUuEdExgNFQCZwg6fK41TrpqYGqKmdU1M7H2h659TUzgea3jm5fT6NLoW1UkqputesUlgrpZSqmAYDpZRSzScYVJcao7ERkSQR2eBM41Hzpd+8SERmi8hBEdlYZlu4iHwrItud3+t2bT8PquR8ZorInjIpVy7yZhndJSLRIrJYRDaLyCYRude5vVF+TlWcT6P9nEQkQERWisg65zn9xbm9m4iscF7z3ncO5Kn8OM2hz8CZGmMbZVJjAFdXkBqj0RCRJCDeGNNoJ8o4JxvmAHONMf2d2/4JZBpjnnQG7TBjzEPeLKerKjmfmUCOMeZpb5atppwj/joaY1aLSGsgEbgMO9ij0X1OVZzPVTTSz0ls3vAgY0yOiPgDPwP3Ag8AHxpj5onIK8A6Y8zLlR2nudQMNDVGA2SMWYIdRVbWBOzkQ5zfL6vXQtVCJefTqBlj9hljVjt/Pood/t2ZRvo5VXE+jZaxcpwP/Z1fBjuBd4Fze7WfUXMJBhWlxmjUfwDYD/sbEUl0putoKiKNMfucP+8HIr1ZmDpyl4isdzYjNYrmlIqISFdgMLCCJvA5lTsfaMSfk4j4isha4CDwLbATyDLGFDl3qfaa11yCQVN0pjFmCHAhcKeziaJJMbYNs7G3Y74M9AAGAfuAZ7xbnJoRkWBgIXCfMeZI2eca4+dUwfk06s/JGFNsjBmEzfQwDOjj7jGaSzCoNjVGY2OM2eP8fhD4CPsH0BQcKJmZ7vx+0MvlqRVjzAHnP6oDeJVG+Dk526EXAu8YYz50bm60n1NF59MUPicAY0wWsBg4AwgVkZKJxdVe85pLMKg2NUZjIiJBzs4vRCQI+A2wsepXNRqfAtc7f74e+MSLZam1cilXLqeRfU7OzsnXgS3GmH+VeapRfk6VnU9j/pxEpJ2IhDp/DsQOlNmCDQpXOner9jNqFqOJAJxDxf7NidQYT3i5SDUmIt2xtQGwKUXebYznIyLvAaOx6XYPAI8CHwMfADFAMnCVMaZRdMpWcj6jsU0PBkgCbi3T1t7giciZwE/ABsDh3DwD287e6D6nKs7nahrp5yQiA7AdxL7YG/wPjDGPOa8T84BwYA0wtUxi0FOP01yCgVJKqco1l2YipZRSVdBgoJRSSoOBUkopDQZKKaXQYKCUUgoNBkqdQkSKy2SvXFuXWW5FpGvZrKZKNRQeW/ZSqUYs1zm1X6lmQ2sGSrnIuYbEP53rSKwUkZ7O7V1FZJEzydn3IhLj3B4pIh8588yvE5GRzkP5isirztzz3zhnjSrlVRoMlDpVYLlmosllnss2xsQBL2BntAP8B5hjjBkAvAM879z+PPCjMWYgMATY5NzeC3jRGBMLZAETPXw+SlVLZyArVY6I5BhjgivYngSMMcbsciY722+MiRCRQ9gFUwqd2/cZY9qKSDoQVTYFgDNt8rfGmF7Oxw8B/saYxz1/ZkpVTmsGSrnHVPKzO8rmhylG++5UA6DBQCn3TC7zfZnz56XYTLgA12IToQF8D9wOpYuPhNRXIZVyl96RKHWqQOeqUSW+MsaUDC8NE5H12Lv7q53b7gbeEJE/AOnANOf2e4FZInITtgZwO3bhFKUaHO0zUMpFzj6DeGPMIW+XRam6ps1ESimltGaglFJKawZKKaXQYKCUUgoNBkoppdBgoJRSCg0GSimlgP8HogRq5+C4VmwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGmX3JPzhoM0",
        "outputId": "c9620e20-32fe-412b-d5e6-bfa161937f30"
      },
      "source": [
        "#test data\n",
        "\n",
        "with open(\"s4.pkl\", \"rb\") as infile:\n",
        "    big_data = pickle.load(infile)\n",
        "\n",
        "print([i for i in big_data[:, 1]]) #use these to decode with a simple if loop ->  not super sure if each stim represents one flash\n",
        "\n",
        "'''\n",
        "'''\n",
        "\n",
        "# train_data = train_data[:(len(train_data)*10//100)]\n",
        "\n",
        "test_data = np.array([i for i in big_data[:, 0]])\n",
        "test_labels = np.array([i for i in big_data[:, 1]])\n",
        "test_stim = np.array([i for i in big_data[:, 2]])\n",
        "\n",
        "# with open(f\"labels.pkl\", \"wb\") as outfile:\n",
        "#   pickle.dump([T_labels, V_labels], outfile)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqbKwICO5_au",
        "outputId": "e244bd74-6fe4-4f33-8fea-9569cc719552"
      },
      "source": [
        "def standardize_data(arr):\n",
        "         \n",
        "    '''\n",
        "    This function standardize an array, its substracts mean value, \n",
        "    and then divide the standard deviation.\n",
        "    \n",
        "    param 1: array \n",
        "    return: standardized array\n",
        "    '''    \n",
        "    rows, columns = arr.shape\n",
        "    \n",
        "    standardizedArray = np.zeros(shape=(rows, columns)) \n",
        "    tempArray = np.zeros(rows)  \n",
        "    \n",
        "    for column in tqdm(range(columns)):\n",
        "        \n",
        "        # print(column)\n",
        "        mean = np.mean(X[:,column]) #mean of channel\n",
        "        std = np.std(X[:,column]) #std of channel\n",
        "        tempArray = np.empty(0)\n",
        "        \n",
        "        for element in X[:,column]: \n",
        "            \n",
        "            tempArray = np.append(tempArray, ((element - mean) / std)) #row val - mean/std = mean of 0, STD of 1\n",
        " \n",
        "        standardizedArray[:,column] = tempArray\n",
        "    \n",
        "    return standardizedArray\n",
        "\n",
        "PCA_test = []\n",
        "\n",
        "# Standardizing data\n",
        "X = test_data.reshape(len(test_data)*8, 175) #flatten axis for standardizing\n",
        "X = standardize_data(X).reshape(-1, 8, 175)\n",
        "# print(X.shape)\n",
        "\n",
        "for batch in tqdm(X):\n",
        "  # Calculating the covariance matrix\n",
        "  covariance_matrix = np.cov(batch.T)\n",
        "  # print(covariance_matrix)\n",
        "\n",
        "  # Using np.linalg.eig function\n",
        "  eigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)\n",
        "  # print(\"Eigenvector: \\n\",eigen_vectors,\"\\n\")\n",
        "  # print(\"Eigenvalues: \\n\", eigen_values, \"\\n\")\n",
        "\n",
        "# Calculating the explained variance on each of components\n",
        "  variance_explained = []\n",
        "  for i in eigen_values:\n",
        "      variance_explained.append((i/sum(eigen_values))*100)\n",
        "        \n",
        "  # print(variance_explained)\n",
        "\n",
        "# Identifying components that explain at least 95%\n",
        "\n",
        "  cumulative_variance_explained = np.cumsum(variance_explained)\n",
        "  # print(cumulative_variance_explained)\n",
        "\n",
        "# Using two first components (because those explain more than 95%)\n",
        "  projection_matrix = (eigen_vectors.T[:][:60]).T\n",
        "  # print(projection_matrix)\n",
        "\n",
        "# Getting the product of original standardized X and the eigenvectors \n",
        "  batch_pca = batch.dot(projection_matrix)\n",
        "  PCA_test.append(batch_pca)\n",
        "\n",
        "# print(PCA_array)\n",
        "PCA_test = np.array(PCA_test).real\n",
        "print(PCA_test.shape)\n",
        "with open(f\"PCA_test.pkl\", \"wb\") as outfile:\n",
        "  pickle.dump(PCA_test, outfile)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 175/175 [01:09<00:00,  2.51it/s]\n",
            "100%|██████████| 4198/4198 [03:55<00:00, 17.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(4198, 8, 60)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejx4etKk-cMw",
        "outputId": "ca10c0d8-424a-4724-ac39-f0d3a5f44186"
      },
      "source": [
        "with open(\"PCA_test.pkl\", \"rb\") as infile:\n",
        "    PCA_test = pickle.load(infile) \n",
        "\n",
        "print(PCA_test.shape) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4198, 8, 60)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgVJE6tB_jgp",
        "outputId": "45bf70de-90f5-4ba8-beb1-99bd0270572b"
      },
      "source": [
        "PCA_test = PCA_test.reshape(4198, 8, 60, 1)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(PCA_test, test_labels, verbose=2)\n",
        "y_pred = np.argmax(model.predict(PCA_test), axis=-1) #argmax basically just returns the indice with the greatest value -> eg. armax([0.9, 0.1]) returns 0\n",
        "print(str(y_pred[:100])+'\\n')\n",
        "print(test_labels[:100]) #TRY ARGMAX\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "132/132 - 0s - loss: 0.7624 - accuracy: 0.7323\n",
            "[1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
            " 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1]\n",
            "\n",
            "[0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYQ5YY5gw7eQ",
        "outputId": "14761aa0-e6b6-4e7d-d58d-a77d055846a0"
      },
      "source": [
        "#                         1    2    3    4    5    6\n",
        "char_matrix = np.array([['A', 'B', 'C', 'D', 'E', 'F'], # 7\n",
        "                        ['G', 'H', 'I', 'J', 'K', 'L'], # 8\n",
        "                        ['M', 'N', 'O', 'P', 'Q', 'R'], # 9\n",
        "                        ['S', 'T', 'U', 'V', 'W', 'X'], # 10\n",
        "                        ['Y', 'Z', '1', '2', '3', '4'], # 11\n",
        "                        ['5', '6', '7', '8', '9', '0']])# 12\n",
        "\n",
        "print(char_matrix.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHGjBE4UK8r-"
      },
      "source": [
        "#decoder to-do\n",
        "#for every 120 indices in decode array calculate the x, y coordinates with the highest activations\n",
        "#try using confidences as a metric to improve accuracy \n",
        "import collections\n",
        "\n",
        "def decode(y_pred, stim):\n",
        "  words = []\n",
        "  decode_array = np.array([y_pred, stim]).T\n",
        "  new_decode = np.array([decode_array[i:i+120] for i in range(0, len(decode_array), 120)], dtype=object)\n",
        "\n",
        "  for letter in new_decode:\n",
        "    col_count = []\n",
        "    row_count = []\n",
        "    for flash in letter:\n",
        "      if flash[0] == 1:\n",
        "        if flash[1] > 6 and flash[1] <= 12:\n",
        "          col_count.append(flash[1])\n",
        "        if flash[1] <= 6:\n",
        "          row_count.append(flash[1])\n",
        "          \n",
        "    # print(collections.Counter(col_count))\n",
        "\n",
        "    y = max(collections.Counter(col_count), key=collections.Counter(col_count).get)\n",
        "    x = max(collections.Counter(row_count), key=collections.Counter(row_count).get)\n",
        "    words.append(char_matrix[x-1, y-7])\n",
        "\n",
        "  return words\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIvhkCxf2hRB",
        "outputId": "6d4891bf-a536-4f68-cd0a-3ac62cdce2a9"
      },
      "source": [
        "predictions = decode(y_pred, test_stim)\n",
        "answers = decode(test_labels, test_stim)\n",
        "print(answers)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['T', 'O', 'K', 'E', 'N', 'M', 'I', 'R', 'A', 'R', 'J', 'U', 'J', 'U', 'Y', 'M', 'A', 'N', 'S', 'O', 'C', 'I', 'N', 'C', 'O', 'J', 'U', 'E', 'G', 'O', 'Q', 'U', 'E', 'S', 'O']\n",
            "['E', 'P', 'A', 'F', 'Q', 'O', 'I', 'Z', 'C', 'Q', 'V', 'K', 'G', '2', 'M', 'P', 'Y', 'N', 'Z', 'S', '0', 'Q', 'F', 'X', 'F', 'Q', 'Y', 'Z', '8', 'Z', 'G', '1', 'I', 'Z', 'U']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "P4oHlWMf8vD0",
        "outputId": "94fbc2bc-7e35-455b-b2e7-31461d120626"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "cm = confusion_matrix(test_labels, y_pred)\n",
        "print(cm)\n",
        "# or\n",
        "#cm = np.array([[1401,    0],[1112, 0]])\n",
        "\n",
        "plt.imshow(cm, cmap=plt.cm.Blues)\n",
        "plt.xlabel(\"Predicted labels\")\n",
        "plt.ylabel(\"True labels\")\n",
        "plt.xticks([], [])\n",
        "plt.yticks([], [])\n",
        "plt.title('Confusion matrix ')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2920  578]\n",
            " [ 546  154]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAEFCAYAAACLjtDTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXvElEQVR4nO3debBcZZ3G8e9zE0CWsAZigCBRg1RAQYYCxC2KIqBW0FFUKGQYFBcQHbVGREswDFOM44oiGiAjiIJQoESMhBilkBmRBIyBBJWMgEkIhLBvIsHf/HHeHg6Xe7tP33T3vW+f52Oduue8Z3tb4fF937MpIjAzy9XAaFfAzGxDOMTMLGsOMTPLmkPMzLLmEDOzrDnEzCxrDrExRtKmkn4q6WFJl23AcY6SdE0n6zZaJL1W0h9Hux42Nsn3iY2MpCOBTwK7A48CS4AzIuL6DTzu0cDHgAMjYv0GV3SMkxTAtIhYMdp1sTy5JTYCkj4JfB34d2ASsAvwbWBmBw7/IuBPdQiwKiSNH+062BgXEZ7amICtgMeAdzfZZhOKkLs7TV8HNknrZgCrgE8Ba4E1wLFp3ReBvwFPp3McB5wGXFQ69q5AAOPT8j8Bf6ZoDd4BHFUqv76034HAIuDh9PfA0rprgdOB/07HuQaYOMxva9T/X0v1Pxw4DPgT8ABwSmn7/YDfAA+lbb8FbJzWXZd+y+Pp976ndPzPAPcA32+UpX1eks6xT1reEbgPmDHa/2x4Gp1p1CuQ2wQcAqxvhMgw28wCbgB2ALYH/gc4Pa2bkfafBWyU/uV/AtgmrR8cWsOGGLA58AjwsrRuMrBHmv//EAO2BR4Ejk77vS8tb5fWXwv8L7AbsGlaPnOY39ao/xdS/T+YQuSHwARgD+BJYGra/h+AA9J5dwVuAz5ROl4ALx3i+P9B8X8Gm5ZDLG3zQWA5sBkwH/jyaP9z4Wn0Jncn27cdsC6ad/eOAmZFxNqIuI+ihXV0af3Taf3TETGPohXyshHW5+/AnpI2jYg1EbFsiG3eCtweEd+PiPURcTHwB+DtpW3+KyL+FBFPApcCezc559MU439PA5cAE4FvRMSj6fzLgb0AIuKmiLghnfdO4LvA6yv8plMj4qlUn+eIiHOBFcBvKYL7cy2OZ33MIda++4GJLcZqdgTuKi3flcr+/xiDQvAJYIt2KxIRj1N0wT4MrJH0M0m7V6hPo047lZbvaaM+90fEM2m+ETL3ltY/2dhf0m6SrpJ0j6RHKMYRJzY5NsB9EfHXFtucC+wJfDMinmqxrfUxh1j7fgM8RTEONJy7KQboG3ZJZSPxOEW3qeGF5ZURMT8i3kzRIvkDxb/crerTqNPqEdapHedQ1GtaRGwJnAKoxT5NL5lL2oJinPF84DRJ23aiopYnh1ibIuJhivGgsyUdLmkzSRtJOlTSl9JmFwOfl7S9pIlp+4tGeMolwOsk7SJpK+CzjRWSJkmaKWlzimB9jKIrNtg8YDdJR0oaL+k9wHTgqhHWqR0TKMbtHkutxI8MWn8v8OI2j/kNYHFEfAD4GfCdDa6lZcshNgIR8RWKe8Q+TzGovRI4EfhJ2uTfgMXAUuAW4OZUNpJzLQB+lI51E88NnoFUj7sprti9nueHBBFxP/A2iiui91NcWXxbRKwbSZ3a9GngSIqrnudS/Jay04ALJD0k6YhWB5M0k+LiSuN3fhLYR9JRHauxZcU3u5pZ1twSM7OsOcTMLGsOMTPLmkPMzLLWlYdrNX7T0MYTunFo65Lp03Ye7SpYm5Yt/d26iNh+pPuP2/JFEeuf90DEkOLJ++ZHxCEjPVc3dSfENp7AJi9rebXcxpDLf/6l1hvZmLL75M0HP4XRllj/ZOV/T/+65OxWT1mMGr/mxKy2BMp/RMkhZlZXAgbGjXYtNphDzKzO1Oox1rHPIWZWW+5Omlnu3BIzs2wJt8TMLGdyS8zMMuerk2aWLw/sm1nOhLuTZpY5t8TMLF/uTppZzgSM88C+meXMY2Jmli93J80sd26JmVnW3BIzs2zJjx2ZWe782JGZ5csD+2aWO3cnzSxbfp+YmeXN3Ukzy50H9s0sax4TM7Nsyd1JM8udW2JmljP1QYjl35Y0sxEp3k6tSlPLY0lTJP1K0nJJyyR9PJWfJmm1pCVpOqy0z2clrZD0R0lvKZUfkspWSDq51bndEjOrKwkNdKwlth74VETcLGkCcJOkBWnd1yLiy889taYD7wX2AHYEfiFpt7T6bODNwCpgkaS5EbF8uBM7xMxqrFPdyYhYA6xJ849Kug3YqckuM4FLIuIp4A5JK4D90roVEfHnVL9L0rbDhpi7k2Y11kZ3cqKkxaXp+CbH3BV4JfDbVHSipKWS5kjaJpXtBKws7bYqlQ1XPiyHmFmNtRFi6yJi39I0e5jjbQFcDnwiIh4BzgFeAuxN0VL7Sqd/g7uTZnWlNHXqcNJGFAH2g4i4AiAi7i2tPxe4Ki2uBqaUdt85ldGkfEhuiZnVlKjWCqt4dVLA+cBtEfHVUvnk0mbvAG5N83OB90raRNJUYBpwI7AImCZpqqSNKQb/5zY7t1tiZjU2MNCxdsyrgaOBWyQtSWWnAO+TtDcQwJ3AhwAiYpmkSykG7NcDJ0TEMwCSTgTmA+OAORGxrNmJHWJmNdbBq5PXM3TndF6Tfc4AzhiifF6z/QZziJnVVYfHxEaLQ8ysxvrhsSOHmFlNNQb2c+cQM6uxDj52NGocYmZ1JXcnzSxzDjEzy5pDzMyy5YF9M8tf/hnmEDOrLXX0saNR4xAzqzF3J80sb/lnmEPMrM7cEjOzbFV9V9hY5xAzqzGHmJllzc9OmlnW3BIzs3z5AXAzy5mAPsgwh5hZffnqpJllbsAD+2aWLbk7aWYZE26JmVnm3BIzs6x5YN/M8uUxMTPLmZBfimhmeXNLzMyy5jExM8uXx8TMLGfFs5P5p1j+o3pmNmJStan1cTRF0q8kLZe0TNLHU/m2khZIuj393SaVS9JZklZIWippn9Kxjknb3y7pmFbndoiZ1djAgCpNFawHPhUR04EDgBMkTQdOBhZGxDRgYVoGOBSYlqbjgXOgCD3gVGB/YD/g1EbwDfsb2v3RZtYn9Ox79ltNrUTEmoi4Oc0/CtwG7ATMBC5Im10AHJ7mZwIXRuEGYGtJk4G3AAsi4oGIeBBYABzS7NweEzOrqTbfJzZR0uLS8uyImD3kcaVdgVcCvwUmRcSatOoeYFKa3wlYWdptVSobrnxYDjGz2mrrfWLrImLflkeUtgAuBz4REY+Ujx8RISlGVNUm3J00q7FODewXx9JGFAH2g4i4IhXfm7qJpL9rU/lqYEpp951T2XDlw3KImdWVOjewr6LJdT5wW0R8tbRqLtC4wngMcGWp/P3pKuUBwMOp2zkfOFjSNmlA/+BUNix3J81qqsP3ib0aOBq4RdKSVHYKcCZwqaTjgLuAI9K6ecBhwArgCeBYgIh4QNLpwKK03ayIeKDZiR1iZjXWqRCLiOspcnEoBw2xfQAnDHOsOcCcqud2iJnVWB/csO8QM6uzfnjsyCFmVld+ANzMcla8FDH/FHOImdXYQB80xdq6Tyzdu/GKblXGzHqrkze7jpaWISbpWklbpqfLbwbOlfTVVvuZ2dimDj4APpqqtMS2iohHgHdSPHW+P/Cm7lbLzHphQNWmsaxKiI1PzzwdAVzV5fqYWQ918H1io6ZKiM2ieHZpRUQskvRi4PbuVsvMuk0UVyir/Gcsa3l1MiIuAy4rLf8Z+MduVsrMemOMN7IqGTbEJH0TGPbdPxFxUldqZGa9kcGgfRXNWmKLm6wzsz7QBxk2fIhFxAXlZUmbRcQT3a+SmfWCqMnNrpJeJWk58Ie0vJekb3e9ZmbWdXW5Ovl1ii+Q3A8QEb8HXtfNSplZ91W9W3+sN9YqPTsZESsHDQA+053qmFkv9UN3skqIrZR0IBDpQwAfp/imnJllLv8Iq9ad/DDFa2R3Au4G9maY18qaWV764dnJKje7rgOO6kFdzKyHiquTo12LDVfl6uSLJf1U0n2S1kq6Mj16ZGY5U7Urk/1wdfKHwKXAZGBHikeQLu5mpcysN/qhO1klxDaLiO9HxPo0XQS8oNsVM7PuanQnc38VT7NnJ7dNsz+XdDJwCcWzlO+h+PClmWVurLeyqmg2sH8TRWg1fuWHSusC+Gy3KmVmvZF/hDV/dnJqLytiZr0lwbix3lesoNId+5L2BKZTGguLiAu7VSkz641+704CIOlUYAZFiM0DDgWuBxxiZpnrgwyrdHXyXcBBwD0RcSywF7BVV2tlZl0nxICqTWNZle7kkxHxd0nrJW0JrAWmdLleZtZtGbyhoooqIbZY0tbAuRRXLB8DftNshz2m7cwVV3+pA9WzXpmy3WajXQUbBZ0aE5M0B3gbsDYi9kxlpwEfBO5Lm50SEfPSus8Cx1G8EeekiJifyg8BvgGMA86LiDNbnbvKs5MfTbPfkXQ1sGVELK3+88xsLBIwrnNNse8B3+L5Y+Vfi4gvP+e80nTgvcAeFE8B/ULSbmn12cCbgVXAIklzI2J5sxM3u9l1n2brIuLmZgc2s7GvU3dYRMR1knatuPlM4JKIeAq4Q9IKYL+0bkX6ohqSLknbjizEgK80qzPwxooVNrMxqo0Qmyip/PGg2RExu8J+J0p6P8WHhz4VEQ9SvNbrhtI2q1IZwMpB5fu3OkGzm13fUKGCZpap4tXTlVNsXUTs2+YpzgFOp2j0nE7RMPrnNo/RUqWbXc2sP3Xzhv2IuLcxL+lc4Kq0uJrn3uGwcyqjSfmwqtwnZmZ9qpsfCpE0ubT4DuDWND8XeK+kTSRNBaYBNwKLgGmSpkramGLwf26r87glZlZTAsZ37haLiyme7JkoaRVwKjBD0t4U3ck7SS+RiIhlki6lGLBfD5wQEc+k45wIzKe4xWJORCxrde4qjx2J4vXUL46IWZJ2AV4YETe2+0PNbGzp1B0WEfG+IYrPb7L9GcAZQ5TPo81XfVXpTn4beBXQqOSjFPdymFnGVPGRo3547Gj/iNhH0u8AIuLB1F81s8yN8XyqpEqIPS1pHEW/FknbA3/vaq3MrCf64HVilULsLODHwA6SzqB4q8Xnu1orM+s6UZOXIkbEDyTdRPE6HgGHR4S/AG6Wuww+AlJFlauTuwBPAD8tl0XEX7pZMTPrPvXBW/ardCd/xrMfDHkBMBX4I8UT6GaWqX75AniV7uTLy8vp7RYfHWZzM8tILUJssIi4WVLLJ8vNbOyry4dCPllaHAD2Ae7uWo3MrCeKT7aNdi02XJWW2ITS/HqKMbLLu1MdM+ulsX43fhVNQyzd5DohIj7do/qYWY/0/cC+pPERsV7Sq3tZITPrnT5oiDVtid1IMf61RNJc4DLg8cbKiLiiy3Uzs64SAzW5T+wFwP0U79Rv3C8WgEPMLGOi/1tiO6Qrk7fybHg1RFdrZWbdJxjfB4NizUJsHLAFDNnedIiZZa4OLbE1ETGrZzUxs57r91ss8v91ZtZUH2RY0xA7qGe1MLOeE/3xubNmH899oJcVMbMeU/93J82sjxV37DvEzCxj+UeYQ8ys1vqgIeYQM6sv1eN9YmbWn/r+6qSZ9T8P7JtZvlST11ObWX9yd9LMstcPLbF+CGIzGyFVnFoeR5ojaa2kW0tl20paIOn29HebVC5JZ0laIWlp+gxkY59j0va3Szqmym9wiJnVlIBxUqWpgu8BhwwqOxlYGBHTgIVpGeBQYFqajgfOgSL0gFOB/YH9gFMbwdeMQ8ysxqRqUysRcR0w+HnrmcAFaf4C4PBS+YVRuAHYWtJk4C3Agoh4ICIeBBbw/GB8Ho+JmdWWUPUHjyZKWlxanh0Rs1vsMyki1qT5e4BJaX4nYGVpu1WpbLjyphxiZjXWxrj+uojYd6TniYiQ1JU3Qrs7aVZTxS0WqjSN0L2pm0j6uzaVrwamlLbbOZUNV96UQ8ysriqOh23AXRhzgcYVxmOAK0vl709XKQ8AHk7dzvnAwZK2SQP6B6eyptydNKuxTj12JOliYAbF2NkqiquMZwKXSjoOuAs4Im0+DzgMWAE8ARwLxYtYJZ0OLErbzaryclaHmFlNFS9F7MyxIuJ9w6x63mvuIyKAE4Y5zhxgTjvndoiZ1VgbVyfHLIeYWY31wVNHDjGzOnNLzMyy1ckxsdHkEDOrK8kvRTSzvOUfYQ4xs9rydyfNLHv5R5hDzKze+iDFHGJmNebupJllLf8Ic4iZ1VsfpJhDzKymio+A5J9iDjGzutqwd4WNGQ4xsxrrgwxziJnVl/ri47kOMbMa64MMc4iZ1VXVr3uPdQ4xszrrgxRziJnVmG+xMLOseUzMzPLl+8TMLHfuTppZtoRbYmaWuT7IMIeYWa31QYo5xMxqzC9FNLOs5R9hDjGzeuuDFHOImdVUv7wUcWC0K2BmoyTd7FplqnQ46U5Jt0haImlxKttW0gJJt6e/26RySTpL0gpJSyXtM9Kf4RAzqzFVnNrwhojYOyL2TcsnAwsjYhqwMC0DHApMS9PxwDkj/Q0OMbPaKl6KWGXaADOBC9L8BcDhpfILo3ADsLWkySM5gUPMrMY62Z0EArhG0k2Sjk9lkyJiTZq/B5iU5ncCVpb2XZXK2uaBfbOaarOrOLExzpXMjojZg7Z5TUSslrQDsEDSH8orIyIkxUjrOxyHmFmdVU+xdaVxriFFxOr0d62kHwP7AfdKmhwRa1J3cW3afDUwpbT7zqmsbe5OmtWYKv6n5XGkzSVNaMwDBwO3AnOBY9JmxwBXpvm5wPvTVcoDgIdL3c62uCVmVmMdfOpoEvDjdBFgPPDDiLha0iLgUknHAXcBR6Tt5wGHASuAJ4BjR3pih5hZXQkGOhRiEfFnYK8hyu8HDhqiPIATOnFuh5hZreV/x75DzKym/FJEM8teH2SYQ8ysztwSM7OsbeAjRWOCQ8ysxvKPMIeYWW21+VzkmOUQM6uxfngpokPMrM7yzzCHmFmd9UGGOcTM6kv+ZJuZ5atf7tj3q3jMLGtuiZnVWD+0xBxiZjXmWyzMLF++2dXMctYvA/sOMbMac3fSzLLmlpiZZa0PMswhZlZrfZBiDjGzmhL0xWNHKr6c1OGDSvdRfGPOzLrnRRGx/Uh3lnQ1MLHi5usi4pCRnqubuhJiZma94mcnzSxrDjEzy5pDzMyy5hDrIUnPSFoi6VZJl0nabAOO9T1J70rz50ma3mTbGZIOHME57pT0vIHf4coHbfNYm+c6TdKn262jmUOst56MiL0jYk/gb8CHyysljeiWl4j4QEQsb7LJDKDtEDPLgUNs9PwaeGlqJf1a0lxguaRxkv5T0iJJSyV9CECFb0n6o6RfADs0DiTpWkn7pvlDJN0s6feSFkralSIs/yW1Al8raXtJl6dzLJL06rTvdpKukbRM0nlUuBVS0k8k3ZT2OX7Quq+l8oWStk9lL5F0ddrn15J2H+KYJ0lann7/JSP7r9dqIyI89WgCHkt/xwNXAh+haCU9DkxN644HPp/mNwEWA1OBdwILgHHAjsBDwLvSdtcC+wLbAytLx9o2/T0N+HSpHj8EXpPmdwFuS/NnAV9I828FApg4xO+4s1FeOsemwK3Admk5gKPS/BeAb6X5hcC0NL8/8MvBdQTuBjZJ81uP9v9unsb25Dv2e2tTSUvS/K+B8ym6eTdGxB2p/GDgFY3xLmArYBrwOuDiiHgGuFvSL4c4/gHAdY1jRcQDw9TjTcD00ifst5S0RTrHO9O+P5P0YIXfdJKkd6T5Kamu9wN/B36Uyi8CrkjnOBC4rHTuTYY45lLgB5J+AvykQh2sxhxivfVkROxdLkj/Mj9eLgI+FhHzB213WAfrMQAcEBF/HaIulUmaQRGIr4qIJyRdC7xgmM0jnfehwf8dDOGtFIH6duBzkl4eEevbqpzVhsfExp75wEckbQQgaTdJmwPXAe9JY2aTgTcMse8NwOskTU37bpvKHwUmlLa7BvhYY0FSI1SuA45MZYcC27So61bAgynAdqdoCTYMAI3W5JHA9RHxCHCHpHenc0jSXuUDShoApkTEr4DPpHNs0aIeVmMOsbHnPGA5cLOkW4HvUrSYfwzcntZdCPxm8I4RcR/FmNoVkn7Ps925nwLvaAzsAycB+6aB8+U8e5X0ixQhuIyiW/mXFnW9Ghgv6TbgTIoQbXgc2C/9hjcCs1L5UcBxqX7LgJmDjjkOuEjSLcDvgLMi4qEW9bAa87OTZpY1t8TMLGsOMTPLmkPMzLLmEDOzrDnEzCxrDjEzy5pDzMyy9n9peRVuiQyY1gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AowdkQ5POzE7"
      },
      "source": [
        "Without Padding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5YXd0gzUwRE",
        "outputId": "abead9a0-4d76-4eb6-ba19-e51ca39e48da"
      },
      "source": [
        "ink = (5000, 8, 120, 1)\n",
        "ink = tf.random.normal(ink)\n",
        "print(ink.shape)\n",
        "PCA_train = PCA_train.reshape(504, 8, 120, 1)\n",
        "#l2\n",
        "L2 = Conv2D(20, (8, 1), activation=\"relu\", input_shape=(8, 120, 1))(PCA_train)\n",
        "print(L2.shape) #0th index: # of batches, 1st index: height of output, 2nd index: width of output, 3rd index: depth -> # of filters\n",
        "#l3\n",
        "L3_1 = Conv2D(filters=16, kernel_size=(1, 5), strides=5, activation=\"relu\", input_shape=(1, 120, 20))(L2)\n",
        "print(L3_1.shape)\n",
        "\n",
        "L3_2 = Conv2D(filters=16, kernel_size=(1, 10), strides=10, activation=\"relu\", input_shape=(1, 120, 20))(L2)\n",
        "print(L3_2.shape)\n",
        "\n",
        "L3_3 = Conv2D(filters=16, kernel_size=(1, 15), strides=15, activation=\"relu\", input_shape=(1, 120, 20))(L2)\n",
        "print(L3_3.shape)\n",
        "\n",
        "#L4\n",
        "concatted = concatenate([L3_1, L3_2, L3_3], axis=2) # merge the outputs of the two models\n",
        "print(concatted.shape)\n",
        "dp = tf.keras.layers.Dropout(.5)(concatted)\n",
        "print(dp.shape)\n",
        "\n",
        "#L5\n",
        "L5_1 = Conv2D(filters=16, kernel_size=(1, 2), strides=2, activation=\"relu\", input_shape=(1, 44, 16))(dp)\n",
        "print(L5_1.shape)\n",
        "\n",
        "L5_2 = Conv2D(filters=16, kernel_size=(1, 4), strides=4, activation=\"relu\", input_shape=(1, 44, 16))(dp)\n",
        "print(L5_2.shape)\n",
        "\n",
        "L5_3 = Conv2D(filters=16, kernel_size=(1, 11), strides=11, activation=\"relu\", input_shape=(1, 44, 16))(dp)\n",
        "print(L5_3.shape)\n",
        "\n",
        "#L6\n",
        "concatted2 = concatenate([L5_1, L5_2, L5_3], axis=2) # merge the outputs of the two models\n",
        "print(concatted2.shape)\n",
        "dp2 = tf.keras.layers.Dropout(.5)(concatted2)\n",
        "print(dp2.shape)\n",
        "\n",
        "#L7\n",
        "mxpool = layers.MaxPooling2D((2, 2), strides=2)(dp2)\n",
        "print(mxpool.shape)\n",
        "\n",
        "#L8 \n",
        "flat = layers.Flatten()(mxpool)\n",
        "dense = layers.Dense(100, activation='relu')(flat)\n",
        "dense2 = layers.Dense(2, activation='relu')(flat)\n",
        "print(dense.shape)\n",
        "print(dense2.shape)\n",
        "\n",
        "#L9\n",
        "sftymax = layers.Softmax(axis=1)(dense2)\n",
        "print(sftymax)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 8, 120, 1)\n",
            "(504, 1, 120, 20)\n",
            "(504, 1, 24, 16)\n",
            "(504, 1, 12, 16)\n",
            "(504, 1, 8, 16)\n",
            "(504, 1, 44, 16)\n",
            "(504, 1, 44, 16)\n",
            "(504, 1, 22, 16)\n",
            "(504, 1, 11, 16)\n",
            "(504, 1, 4, 16)\n",
            "(504, 1, 37, 16)\n",
            "(504, 1, 37, 16)\n",
            "(504, 0, 18, 16)\n",
            "(504, 100)\n",
            "(504, 2)\n",
            "tf.Tensor(\n",
            "[[0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " ...\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]], shape=(504, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFoUruewOvEr"
      },
      "source": [
        "With Padding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTJdMWLYLiiU",
        "outputId": "a92eea65-ffe0-4ff7-d323-05996f260ffd"
      },
      "source": [
        "ink = (5000, 8, 60, 1)\n",
        "ink = tf.random.normal(ink)\n",
        "print(ink.shape)\n",
        "\n",
        "#l2\n",
        "L2 = Conv2D(20, (8, 1), activation=\"relu\", strides=(8, 1), padding=\"same\", input_shape=(8, 60, 1))(ink)\n",
        "print(L2.shape) #0th index: # of batches, 1st index: height of output, 2nd index: width of output, 3rd index: depth -> # of filters\n",
        "#L3\n",
        "L3_1 = Conv2D(filters=16, kernel_size=(1, 5), strides=(1, 5), padding=\"same\", activation=\"relu\")(L2)\n",
        "print(L3_1.shape)\n",
        "\n",
        "L3_2 = Conv2D(filters=16, kernel_size=(1, 10), strides=(1, 10), padding=\"same\", activation=\"relu\")(L2)\n",
        "print(L3_2.shape)\n",
        "\n",
        "L3_3 = Conv2D(filters=16, kernel_size=(1, 15), strides=(1, 15), padding=\"same\", activation=\"relu\")(L2)\n",
        "print(L3_3.shape)\n",
        "\n",
        "#L4\n",
        "concatted = concatenate([L3_1, L3_2, L3_3], axis=2) # merge the outputs of the two models\n",
        "print(concatted.shape)\n",
        "dp = tf.keras.layers.Dropout(.5)(concatted)\n",
        "print(dp.shape)\n",
        "\n",
        "# #L5\n",
        "# L5_1 = Conv2D(filters=16, kernel_size=(1, 2), strides=(1, 2), padding=\"same\", activation=\"relu\", input_shape=(1, 44, 16))(dp)\n",
        "# print(L5_1.shape)\n",
        "\n",
        "# L5_2 = Conv2D(filters=16, kernel_size=(1, 4), strides=(1, 4), padding=\"same\", activation=\"relu\", input_shape=(1, 44, 16))(dp)\n",
        "# print(L5_2.shape)\n",
        "\n",
        "# L5_3 = Conv2D(filters=16, kernel_size=(1, 11), strides=(1, 11), padding=\"same\", activation=\"relu\", input_shape=(1, 44, 16))(dp)\n",
        "# print(L5_3.shape)\n",
        "\n",
        "# #L6\n",
        "# concatted2 = concatenate([L5_1, L5_2, L5_3], axis=2) # merge the outputs of the two models\n",
        "# print(concatted2.shape)\n",
        "# dp2 = tf.keras.layers.Dropout(.5)(concatted2)\n",
        "# print(dp2.shape)\n",
        "\n",
        "# #L7\n",
        "# mxpool = layers.MaxPooling2D((2, 2), strides=2)(dp2)\n",
        "# print(mxpool.shape)\n",
        "\n",
        "# #L8 \n",
        "# flat = layers.Flatten()(mxpool)\n",
        "# dense = layers.Dense(100, activation='relu')(flat)\n",
        "# dense2 = layers.Dense(2, activation='relu')(flat)\n",
        "# print(dense.shape)\n",
        "# print(dense2.shape)\n",
        "\n",
        "# #L9\n",
        "# sftymax = layers.Softmax(axis=1)(dense2)\n",
        "# print(sftymax)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 8, 60, 1)\n",
            "(5000, 1, 60, 20)\n",
            "(5000, 1, 12, 16)\n",
            "(5000, 1, 6, 16)\n",
            "(5000, 1, 4, 16)\n",
            "(5000, 1, 22, 16)\n",
            "(5000, 1, 22, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhQhMzKxOS7l"
      },
      "source": [
        "# #PCA\n",
        "# #p.s use tft.scale_to_z_score() to normalize\n",
        "# #  NOTE: To properly use PCA, input vector components should be converted to\n",
        "#   # similar units of measurement such that the vectors represent a Euclidean\n",
        "#   # space. If no such conversion is available (e.g. one element represents time,\n",
        "#   # another element distance), the canonical approach is to first apply a\n",
        "#   # transformation to the input data to normalize numerical variances, i.e.\n",
        "#   # `tft.scale_to_z_score()`. Normalization allows PCA to choose output axes that\n",
        "#   # help decorrelate input axes.\n",
        "\n",
        "# ink = (500, 120, 64)\n",
        "# ink = np.zeros((500, 120, 64), dtype=float) #numpy\n",
        "\n",
        "# # ink = (1, 64, 120,)\n",
        "# # ink = tf.random.normal(ink) #tensor\n",
        "\n",
        "# @tf.function\n",
        "# def preprocessing_fn(inputs):\n",
        "#     features = []\n",
        "#     outputs = {}\n",
        "#     # for feature_tensor in inputs.values():\n",
        "#     for feature_tensor in inputs:\n",
        "#         # tf.convert_to_tensor(feature_tensor) #coverts back to tensor if needed\n",
        "#         # standard scaler pre-req for PCA\n",
        "#         features.append(tft.scale_to_z_score(feature_tensor))\n",
        "          \n",
        "#     # concat to make feature matrix for PCA to run over\n",
        "#     feature_matrix = tf.concat(features, axis=1)  \n",
        "    \n",
        "#     # get orthonormal vector matrix\n",
        "#     orthonormal_vectors = tft.pca(feature_matrix, output_dim=2, dtype=tf.float32)\n",
        "    \n",
        "#     # multiply matrix by feature matrix to get projected transformation\n",
        "#     pca_examples = tf.linalg.matmul(feature_matrix, orthonormal_vectors)\n",
        "    \n",
        "#     # unstack and add to output dict\n",
        "#     pca_examples = tf.unstack(pca_examples, axis=1)\n",
        "#     outputs['Principal Component 1'] = pca_examples[0]\n",
        "#     outputs['Principal Component 2'] = pca_examples[1]\n",
        "\n",
        "#     return outputs\n",
        "\n",
        "# preprocessing_fn(ink)\n",
        "  \n",
        "# for batch in ink:\n",
        "#   # k = tft.scale_to_z_score(ink, dtype=tf.float64)\n",
        "#   i = tft.pca(batch[:][:], output_dim=120, dtype=tf.float32)\n",
        "#   print(i)\n",
        "  #RuntimeError: tf.placeholder() is not compatible with eager execution.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvzrMbRRMTZY",
        "outputId": "508b42f0-3889-4131-f56a-b2d23677983d"
      },
      "source": [
        "%matplotlib\n",
        "def butter_lowpass(cutoff, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "    return b, a\n",
        "\n",
        "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
        "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "# Filter requirements.\n",
        "order = 6\n",
        "fs = 250       # sample rate, Hz\n",
        "cutoff = 1.5  # desired cutoff frequency of the filter, Hz\n",
        "\n",
        "# Get the filter coefficients so we can check its frequency response.\n",
        "b, a = butter_lowpass(cutoff, fs, order)\n",
        "\n",
        "def prepare_data(file):\n",
        "  allX = []\n",
        "  allY = []\n",
        "  allT = []\n",
        "  allF = []\n",
        "  raw_data = scipy.io.loadmat(file)\n",
        "  useful_data = raw_data['data'].copy()\n",
        "  X = useful_data['X'] #data\n",
        "  Y = useful_data['y'] #labels\n",
        "  T = useful_data['trial'] #batches\n",
        "  F = useful_data['flash'] #everything\n",
        "  # X_mean = np.mean(X, axis=1)\n",
        "  X_filtered = butter_lowpass_filter(X, cutoff, fs, order)\n",
        "  \n",
        "  return X_filtered, Y, T, F\n",
        "\n",
        "def clean_data(X, Y, flash):\n",
        "  \n",
        "  X_samples = np.array([np.array(X[i[0]:i[0]+351]) for i in flash] )\n",
        "  column    = [i[2] for i in flash]\n",
        "  label     = [i[3] - 1 for i in flash]\n",
        "  \n",
        "  LIMIT = 4080 #the last trial is incomplete\n",
        "  X_selected = np.array(X_samples[:LIMIT])\n",
        "  col_selected = np.array(column[:LIMIT])\n",
        "  label_selected = np.array(label[:LIMIT])\n",
        "\n",
        "  y = np.array(tf.keras.utils.to_categorical(label_selected))\n",
        "\n",
        "  false_idx = [k for k, i in enumerate(y) if i[0] == 1]\n",
        "  true_idx  = [k for k, i in enumerate(y) if i[0] == 0]\n",
        "\n",
        "  falseX = X_selected[false_idx]\n",
        "  falsey = y[false_idx]\n",
        "\n",
        "  trueX  = X_selected[true_idx]  \n",
        "  truey  = y[true_idx]\n",
        "  # proportional data to avoid greedy cost funtion\n",
        "\n",
        "  proportionalX = falseX[:int(len(trueX))]\n",
        "  proportionaly = falsey[:int(len(truey))]\n",
        "\n",
        "  finalX = np.concatenate((trueX, proportionalX))\n",
        "  finaly = np.concatenate((truey, proportionaly))\n",
        "\n",
        "  X_timeseries = np.vstack(finalX)\n",
        "  X_letters = X_timeseries.reshape(34,40,351,8)\n",
        "  y_letters = finaly.reshape(34,40,2)\n",
        "  cleaned_X = np.vstack(X_letters)\n",
        "  cleaned_Y = np.vstack(y_letters)\n",
        "  \n",
        "  return cleaned_X, cleaned_Y\n",
        "\n",
        "  def normalized(vec):\n",
        "    norm_vec = (vec - vec.min(axis=1, keepdims=True))/vec.ptp(axis=1, keepdims=True)\n",
        "    return norm_vec\n",
        "\n",
        "data, Y, T, F = prepare_data('P300S07.mat')\n",
        "print(data[0][0].shape)\n",
        "\n",
        "X_samples = np.array(list(data[0][0][i[0]:i[0]+250] for i in F[0, 0]))\n",
        "print(X_samples.shape)\n",
        "column = [i[2] for i in F[0, 0]]\n",
        "label = [i[3] - 1 for i in F[0, 0]]\n",
        "print(label)\n",
        "print(len(F[0, 0]))\n",
        "print(data[0][0][7000])\n",
        "\n",
        "LIMIT = 4080 #the last trial is incomplete, each dataset has 4200 sample points, each trial(35) consists of a 120 sample points\n",
        "# X_selected = np.array(X_samples[:LIMIT], dtype=float32)\n",
        "# col_selected = np.array(column[:LIMIT])\n",
        "# label_selected = np.array(label[:LIMIT])\n",
        "\n",
        "# y = np.array(tf.keras.utils.to_categorical(label_selected))\n",
        "\n",
        "# false_idx = [k for k, i in enumerate(y) if i[0] == 1]\n",
        "# true_idx  = [k for k, i in enumerate(y) if i[0] == 0]\n",
        "\n",
        "# falseX = X_selected[false_idx]\n",
        "# falsey = y[false_idx]\n",
        "\n",
        "# trueX  = X_selected[true_idx]  \n",
        "# truey  = y[true_idx]\n",
        "# # proportional data to avoid greedy cost funtion\n",
        "\n",
        "# proportionalX = falseX[:int(len(trueX))]\n",
        "# proportionaly = falsey[:int(len(truey))]\n",
        "\n",
        "# finalX = np.concatenate((trueX, proportionalX))\n",
        "# finaly = np.concatenate((truey, proportionaly))\n",
        "\n",
        "# X_timeseries = np.vstack(finalX)\n",
        "# X_letters = X_timeseries.reshape(34,40,351,8)\n",
        "# y_letters = finaly.reshape(34,40,2)\n",
        "# cleaned_X = np.vstack(X_letters)\n",
        "# cleaned_Y = np.vstack(y_letters)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using matplotlib backend: agg\n",
            "(355872, 8)\n",
            "(4200,)\n",
            "[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
            "4200\n",
            "[ 8.08804412e-10  2.85491084e-10 -8.61845008e-11  1.15345464e-10\n",
            "  5.53964017e-10  1.44473611e-10  5.40492291e-10  2.28812085e-10]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}